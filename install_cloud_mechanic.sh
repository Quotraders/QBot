#!/bin/bash
# CLOUD BOT MECHANIC - COMPLETE INSTALLATION
# Date: 2025-09-03 04:57:46 UTC
# User: kevinsuero072897-collab
# Version: CLOUD 3.0 ULTIMATE

echo "================================================"
echo "üå©Ô∏è CLOUD BOT MECHANIC - ULTIMATE INSTALL"
echo "================================================"
echo "This will install the FULL CLOUD monitoring system"
echo "Runs 24/7 on GitHub Actions"
echo "Monitors, repairs, and optimizes ALL workflows"
echo "================================================"

# Create all cloud directories
mkdir -p .github/workflows
mkdir -p Intelligence/mechanic/cloud/{analyzers,optimizers,repair,monitor}
mkdir -p Intelligence/mechanic/cloud/database

# ============================================
# MAIN CLOUD MECHANIC WORKFLOW
# ============================================

cat > .github/workflows/cloud_bot_mechanic.yml << 'EOF'
name: üå©Ô∏è Cloud Bot Mechanic (Ultimate)

on:
  schedule:
    - cron: '*/10 * * * *'  # Every 10 minutes
    - cron: '0 * * * *'     # Every hour for deep scan
    - cron: '0 */6 * * *'   # Every 6 hours for optimization
  workflow_dispatch:
    inputs:
      mode:
        description: 'Operation mode'
        required: false
        default: 'auto'
        type: choice
        options:
          - auto
          - deep_scan
          - optimize
          - repair_all
          - emergency
      target:
        description: 'Target workflow (optional)'
        required: false
        type: string

env:
  MECHANIC_VERSION: '3.0-CLOUD'
  MAX_REPAIR_ATTEMPTS: 3
  ENABLE_AUTO_FIX: true
  OPTIMIZE_MINUTES: true

jobs:
  cloud-mechanic:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    permissions:
      contents: write
      actions: read
      issues: write
      pull-requests: write

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for analysis
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Setup Python Environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install Dependencies
      run: |
        pip install --upgrade pip
        pip install pandas numpy scikit-learn
        pip install requests pyyaml gitpython
        pip install matplotlib seaborn
        pip install pytz croniter

    - name: Run Cloud Mechanic Core
      id: mechanic
      run: |
        python Intelligence/mechanic/cloud/cloud_mechanic_core.py
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        OPERATION_MODE: ${{ inputs.mode || 'auto' }}
        TARGET_WORKFLOW: ${{ inputs.target }}
        REPO_OWNER: ${{ github.repository_owner }}
        REPO_NAME: ${{ github.event.repository.name }}

    - name: Analyze Workflow Health
      if: always()
      run: |
        python Intelligence/mechanic/cloud/workflow_analyzer.py

    - name: Optimize Workflows
      if: steps.mechanic.outputs.needs_optimization == 'true'
      run: |
        python Intelligence/mechanic/cloud/workflow_optimizer.py

    - name: Apply Repairs
      if: steps.mechanic.outputs.repairs_needed == 'true'
      run: |
        python Intelligence/mechanic/cloud/repair_system.py

    - name: Generate Reports
      if: always()
      run: |
        python Intelligence/mechanic/cloud/report_generator.py

    - name: Check Critical Issues
      id: critical_check
      if: always()
      run: |
        if [ -f "Intelligence/mechanic/cloud/alerts/critical.json" ]; then
          echo "has_critical=true" >> $GITHUB_OUTPUT
          echo "alert_title=$(jq -r '.title' Intelligence/mechanic/cloud/alerts/critical.json)" >> $GITHUB_OUTPUT
        else
          echo "has_critical=false" >> $GITHUB_OUTPUT
        fi

    - name: Create Issue for Critical Problems
      if: steps.critical_check.outputs.has_critical == 'true'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const alertData = JSON.parse(fs.readFileSync('Intelligence/mechanic/cloud/alerts/critical.json'));

          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `üö® Cloud Mechanic: ${alertData.title}`,
            body: alertData.body,
            labels: ['cloud-mechanic', 'critical', 'auto-detected']
          });

    - name: Commit Repairs and Optimizations
      if: steps.mechanic.outputs.changes_made == 'true'
      run: |
        git config --global user.email "cloud-mechanic@bot.com"
        git config --global user.name "Cloud Bot Mechanic"

        git add -A
        git diff --cached --quiet || git commit -m "üå©Ô∏è Cloud Mechanic: Auto-repairs and optimizations

        $(cat Intelligence/mechanic/cloud/logs/commit_message.txt 2>/dev/null || echo 'Routine maintenance')"

        git push

    - name: Upload Mechanic Artifacts
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: cloud-mechanic-reports-${{ github.run_number }}
        path: |
          Intelligence/mechanic/cloud/database/
          Intelligence/mechanic/cloud/reports/
          Intelligence/mechanic/cloud/alerts/
        retention-days: 30

    - name: Send Discord/Slack Notification
      if: steps.critical_check.outputs.has_critical == 'true'
      run: |
        # Add your webhook URL here if you want notifications
        echo "Critical issue detected: ${{ steps.critical_check.outputs.alert_title }}"
EOF

# ============================================
# CLOUD MECHANIC CORE ENGINE
# ============================================

cat > Intelligence/mechanic/cloud/cloud_mechanic_core.py << 'EOF'
#!/usr/bin/env python3
"""
CLOUD BOT MECHANIC CORE - Ultimate System
Monitors, analyzes, repairs, and optimizes everything in the cloud
"""

import os
import sys
import json
import yaml
import subprocess
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import re
import hashlib
import traceback
from croniter import croniter
import pytz

class CloudBotMechanic:
    def __init__(self):
        self.version = "3.0-CLOUD-ULTIMATE"
        self.start_time = datetime.utcnow()
        self.github_token = os.environ.get('GITHUB_TOKEN', '')
        self.repo_owner = os.environ.get('REPO_OWNER', '')
        self.repo_name = os.environ.get('REPO_NAME', '')
        self.operation_mode = os.environ.get('OPERATION_MODE', 'auto')

        # Paths
        self.base_path = Path.cwd()
        self.workflow_path = Path('.github/workflows')
        self.cloud_path = Path('Intelligence/mechanic/cloud')
        self.db_path = self.cloud_path / 'database'

        # Create directories
        self.db_path.mkdir(parents=True, exist_ok=True)
        (self.cloud_path / 'reports').mkdir(exist_ok=True)
        (self.cloud_path / 'alerts').mkdir(exist_ok=True)
        (self.cloud_path / 'logs').mkdir(exist_ok=True)

        # Databases
        self.workflow_db = self.db_path / 'workflows.json'
        self.performance_db = self.db_path / 'performance.json'
        self.issues_db = self.db_path / 'issues.json'
        self.repairs_db = self.db_path / 'repairs.json'

        # Load databases
        self.workflows = self.load_json(self.workflow_db, {})
        self.performance = self.load_json(self.performance_db, {})
        self.issues = self.load_json(self.issues_db, [])
        self.repairs = self.load_json(self.repairs_db, [])

        # GitHub API headers
        self.headers = {
            'Authorization': f'token {self.github_token}',
            'Accept': 'application/vnd.github.v3+json'
        }

        # Workflow optimization patterns
        self.optimization_patterns = {
            'cache_dependencies': True,
            'parallel_jobs': True,
            'conditional_steps': True,
            'matrix_strategy': True,
            'artifact_compression': True,
            'shallow_clone': True
        }

        # Critical workflows that must always work
        self.critical_workflows = [
            'ultimate_ml_rl_intel_system.yml',
            'es_nq_critical_trading.yml',
            'options_flow_analysis.yml',
            'cloud_bot_mechanic.yml'
        ]

        print(f"üå©Ô∏è Cloud Bot Mechanic v{self.version} initialized")
        print(f"üìç Repository: {self.repo_owner}/{self.repo_name}")
        print(f"üîß Mode: {self.operation_mode}")

    def load_json(self, path: Path, default: Any) -> Any:
        """Load JSON with default fallback"""
        try:
            if path.exists():
                with open(path, 'r') as f:
                    return json.load(f)
        except:
            pass
        return default

    def save_json(self, path: Path, data: Any):
        """Save JSON data"""
        with open(path, 'w') as f:
            json.dump(data, f, indent=2, default=str)

    # ========================================
    # WORKFLOW ANALYSIS
    # ========================================

    def analyze_all_workflows(self) -> Dict:
        """Deep analysis of all workflows"""
        print("\nüîç ANALYZING ALL WORKFLOWS...")

        analysis = {
            'timestamp': datetime.utcnow().isoformat(),
            'total_workflows': 0,
            'healthy_workflows': 0,
            'broken_workflows': [],
            'slow_workflows': [],
            'expensive_workflows': [],
            'schedule_conflicts': [],
            'optimization_opportunities': [],
            'total_monthly_minutes': 0,
            'issues_found': []
        }

        # Get all workflow files
        workflow_files = list(self.workflow_path.glob('*.yml')) + list(self.workflow_path.glob('*.yaml'))
        analysis['total_workflows'] = len(workflow_files)

        for wf_file in workflow_files:
            self._analyze_workflow(wf_file, analysis)

        # Analyze workflow runs from GitHub API
        self._analyze_workflow_runs(analysis)

        # Check for schedule conflicts
        self._check_schedule_conflicts(analysis)

        # Calculate total minute usage
        self._calculate_minute_usage(analysis)

        # Generate optimization recommendations
        self._generate_optimizations(analysis)

        # Save analysis
        self.workflows['last_analysis'] = analysis
        self.save_json(self.workflow_db, self.workflows)

        self._print_analysis_report(analysis)

        return analysis

    def _analyze_workflow(self, wf_file: Path, analysis: Dict):
        """Analyze individual workflow file"""
        try:
            with open(wf_file, 'r') as f:
                workflow = yaml.safe_load(f)

            wf_name = wf_file.name
            wf_data = {
                'name': workflow.get('name', wf_name),
                'file': wf_name,
                'issues': [],
                'optimizations': [],
                'schedules': [],
                'estimated_minutes': 0,
                'jobs': {}
            }

            # Check workflow structure
            if not workflow:
                wf_data['issues'].append('Empty workflow file')
                analysis['broken_workflows'].append(wf_name)
            elif 'jobs' not in workflow:
                wf_data['issues'].append('No jobs defined')
                analysis['broken_workflows'].append(wf_name)
            else:
                # Analyze triggers
                self._analyze_triggers(workflow.get('on', {}), wf_data)

                # Analyze jobs
                for job_name, job_config in workflow.get('jobs', {}).items():
                    self._analyze_job(job_name, job_config, wf_data)

                # Check for optimization opportunities
                self._check_optimizations(workflow, wf_data)

                if not wf_data['issues']:
                    analysis['healthy_workflows'] += 1

            # Store workflow data
            self.workflows[wf_name] = wf_data

            # Collect issues and optimizations
            if wf_data['issues']:
                for issue in wf_data['issues']:
                    analysis['issues_found'].append(f"{wf_name}: {issue}")

            if wf_data['optimizations']:
                analysis['optimization_opportunities'].extend(
                    [f"{wf_name}: {opt}" for opt in wf_data['optimizations']]
                )

        except yaml.YAMLError as e:
            analysis['broken_workflows'].append(wf_file.name)
            analysis['issues_found'].append(f"{wf_file.name}: YAML parse error - {e}")
        except Exception as e:
            analysis['issues_found'].append(f"{wf_file.name}: Analysis error - {e}")

    def _analyze_triggers(self, triggers: Dict, wf_data: Dict):
        """Analyze workflow triggers"""
        if isinstance(triggers, dict):
            # Check schedules
            if 'schedule' in triggers:
                for schedule in triggers['schedule']:
                    if 'cron' in schedule:
                        cron_expr = schedule['cron']
                        wf_data['schedules'].append(cron_expr)

                        # Validate cron expression
                        try:
                            croniter(cron_expr)

                            # Calculate runs per month
                            runs_per_month = self._calculate_cron_runs(cron_expr)
                            wf_data['runs_per_month'] = runs_per_month
                        except:
                            wf_data['issues'].append(f"Invalid cron: {cron_expr}")

            # Check other triggers
            if 'push' in triggers and not triggers.get('push', {}).get('branches'):
                wf_data['issues'].append('Push trigger without branch filter')

            if 'pull_request' in triggers and not triggers.get('pull_request', {}).get('branches'):
                wf_data['optimizations'].append('Add branch filter to pull_request trigger')

    def _analyze_job(self, job_name: str, job_config: Dict, wf_data: Dict):
        """Analyze individual job"""
        job_data = {
            'timeout': job_config.get('timeout-minutes', 360),
            'runs_on': job_config.get('runs-on', 'ubuntu-latest'),
            'steps': len(job_config.get('steps', [])),
            'has_cache': False,
            'has_error_handling': False,
            'estimated_minutes': 5  # Default estimate
        }

        # Check timeout
        if job_data['timeout'] == 360:  # Default GitHub timeout
            wf_data['optimizations'].append(f"Set explicit timeout for job '{job_name}'")
        elif job_data['timeout'] > 60:
            wf_data['optimizations'].append(f"Job '{job_name}' has long timeout ({job_data['timeout']}m)")

        # Check for caching
        for step in job_config.get('steps', []):
            if isinstance(step, dict):
                # Check for cache action
                if step.get('uses', '').startswith('actions/cache'):
                    job_data['has_cache'] = True

                # Check for error handling
                if step.get('continue-on-error') or step.get('if'):
                    job_data['has_error_handling'] = True

                # Check for setup-python with cache
                if 'setup-python' in step.get('uses', '') and 'cache' in str(step.get('with', {})):
                    job_data['has_cache'] = True

        # Optimization recommendations
        if not job_data['has_cache'] and job_data['steps'] > 5:
            wf_data['optimizations'].append(f"Add caching to job '{job_name}'")

        if not job_data['has_error_handling'] and job_name in ['critical', 'trading', 'ml']:
            wf_data['issues'].append(f"No error handling in critical job '{job_name}'")

        # Estimate minutes
        job_data['estimated_minutes'] = min(job_data['timeout'], job_data['steps'] * 2)
        wf_data['estimated_minutes'] += job_data['estimated_minutes']

        wf_data['jobs'][job_name] = job_data

    def _check_optimizations(self, workflow: Dict, wf_data: Dict):
        """Check for optimization opportunities"""
        # Check for matrix strategy opportunity
        jobs = workflow.get('jobs', {})
        similar_jobs = []

        for job_name, job_config in jobs.items():
            if 'matrix' not in job_config.get('strategy', {}):
                # Look for similar job patterns
                if any(pattern in job_name for pattern in ['test', 'build', 'deploy']):
                    similar_jobs.append(job_name)

        if len(similar_jobs) > 2:
            wf_data['optimizations'].append(f"Consider matrix strategy for {similar_jobs}")

        # Check for parallel jobs opportunity
        sequential_jobs = 0
        for job_name, job_config in jobs.items():
            if 'needs' in job_config:
                sequential_jobs += 1

        if len(jobs) > 3 and sequential_jobs == len(jobs) - 1:
            wf_data['optimizations'].append("Consider parallelizing independent jobs")

        # Check for artifact usage
        has_artifacts = False
        for job_name, job_config in jobs.items():
            for step in job_config.get('steps', []):
                if isinstance(step, dict) and 'artifact' in step.get('uses', ''):
                    has_artifacts = True
                    break

        if len(jobs) > 1 and not has_artifacts:
            wf_data['optimizations'].append("Consider using artifacts to share data between jobs")

    def _calculate_cron_runs(self, cron_expr: str) -> int:
        """Calculate number of runs per month for cron expression"""
        try:
            now = datetime.utcnow()
            cron = croniter(cron_expr, now)

            # Count runs in next 30 days
            runs = 0
            end_date = now + timedelta(days=30)

            while True:
                next_run = cron.get_next(datetime)
                if next_run > end_date:
                    break
                runs += 1

                if runs > 10000:  # Safety limit
                    break

            return runs
        except:
            return 0

    def _analyze_workflow_runs(self, analysis: Dict):
        """Analyze actual workflow runs via GitHub API"""
        if not self.github_token:
            print("‚ö†Ô∏è No GitHub token, skipping API analysis")
            return

        try:
            # Get workflow runs from last 7 days
            url = f"https://api.github.com/repos/{self.repo_owner}/{self.repo_name}/actions/runs"
            params = {
                'created': f'>={(datetime.utcnow() - timedelta(days=7)).strftime("%Y-%m-%d")}',
                'per_page': 100
            }

            response = requests.get(url, headers=self.headers, params=params)

            if response.status_code == 200:
                runs_data = response.json()

                # Analyze runs
                for run in runs_data.get('workflow_runs', []):
                    wf_name = run.get('path', '').split('/')[-1]

                    if wf_name in self.workflows:
                        wf_data = self.workflows[wf_name]

                        # Track performance
                        if 'runs' not in wf_data:
                            wf_data['runs'] = []

                        run_info = {
                            'id': run['id'],
                            'status': run['status'],
                            'conclusion': run.get('conclusion'),
                            'created_at': run['created_at'],
                            'updated_at': run['updated_at'],
                            'run_time_minutes': self._calculate_run_time(run)
                        }

                        wf_data['runs'].append(run_info)

                        # Check for failures
                        if run.get('conclusion') == 'failure':
                            analysis['issues_found'].append(
                                f"{wf_name}: Failed run at {run['created_at']}"
                            )

                        # Check for slow runs
                        if run_info['run_time_minutes'] > 30:
                            if wf_name not in analysis['slow_workflows']:
                                analysis['slow_workflows'].append(wf_name)
            else:
                print(f"‚ö†Ô∏è GitHub API returned {response.status_code}")

        except Exception as e:
            print(f"‚ö†Ô∏è Error analyzing workflow runs: {e}")

    def _calculate_run_time(self, run: Dict) -> float:
        """Calculate workflow run time in minutes"""
        try:
            created = datetime.fromisoformat(run['created_at'].replace('Z', '+00:00'))
            updated = datetime.fromisoformat(run['updated_at'].replace('Z', '+00:00'))
            duration = (updated - created).total_seconds() / 60
            return round(duration, 2)
        except:
            return 0

    def _check_schedule_conflicts(self, analysis: Dict):
        """Check for workflow schedule conflicts"""
        schedule_map = {}

        for wf_name, wf_data in self.workflows.items():
            for schedule in wf_data.get('schedules', []):
                try:
                    cron = croniter(schedule, datetime.utcnow())

                    # Get next 10 runs
                    for _ in range(10):
                        next_run = cron.get_next(datetime)
                        hour_slot = next_run.strftime("%H:00")

                        if hour_slot not in schedule_map:
                            schedule_map[hour_slot] = []

                        schedule_map[hour_slot].append(wf_name)
                except:
                    pass

        # Find conflicts (more than 3 workflows in same hour)
        for hour, workflows in schedule_map.items():
            unique_workflows = list(set(workflows))
            if len(unique_workflows) > 3:
                analysis['schedule_conflicts'].append({
                    'hour': hour,
                    'workflows': unique_workflows,
                    'count': len(unique_workflows)
                })

    def _calculate_minute_usage(self, analysis: Dict):
        """Calculate total minute usage"""
        total_minutes = 0

        for wf_name, wf_data in self.workflows.items():
            runs_per_month = wf_data.get('runs_per_month', 0)
            estimated_minutes = wf_data.get('estimated_minutes', 5)

            workflow_monthly_minutes = runs_per_month * estimated_minutes
            total_minutes += workflow_monthly_minutes

            # Flag expensive workflows
            if workflow_monthly_minutes > 1000:
                analysis['expensive_workflows'].append({
                    'workflow': wf_name,
                    'monthly_minutes': workflow_monthly_minutes,
                    'runs_per_month': runs_per_month,
                    'minutes_per_run': estimated_minutes
                })

        analysis['total_monthly_minutes'] = total_minutes

        # Check against limit
        if total_minutes > 20000:
            analysis['issues_found'].append(
                f"CRITICAL: Estimated {total_minutes} minutes exceeds 20,000 limit!"
            )
        elif total_minutes > 18000:
            analysis['issues_found'].append(
                f"WARNING: Using {total_minutes}/20,000 minutes (90%)"
            )

    def _generate_optimizations(self, analysis: Dict):
        """Generate optimization recommendations"""
        # Recommend optimizations for expensive workflows
        for expensive in analysis['expensive_workflows']:
            wf_name = expensive['workflow']

            if expensive['runs_per_month'] > 100:
                analysis['optimization_opportunities'].append(
                    f"{wf_name}: Reduce frequency (runs {expensive['runs_per_month']} times/month)"
                )

            if expensive['minutes_per_run'] > 15:
                analysis['optimization_opportunities'].append(
                    f"{wf_name}: Optimize runtime (takes {expensive['minutes_per_run']}m per run)"
                )

        # General optimizations
        if analysis['total_monthly_minutes'] > 15000:
            analysis['optimization_opportunities'].insert(0,
                "HIGH PRIORITY: Implement caching across all workflows"
            )
            analysis['optimization_opportunities'].insert(1,
                "HIGH PRIORITY: Use matrix strategies to combine similar jobs"
            )

    # ========================================
    # AUTO-REPAIR SYSTEM
    # ========================================

    def repair_broken_workflows(self) -> Dict:
        """Repair broken workflows"""
        print("\nüîß REPAIRING BROKEN WORKFLOWS...")

        repairs = {
            'timestamp': datetime.utcnow().isoformat(),
            'repaired': [],
            'failed': [],
            'changes_made': False
        }

        for wf_name, wf_data in self.workflows.items():
            if wf_data.get('issues'):
                print(f"  Repairing {wf_name}...")

                if self._repair_workflow(wf_name, wf_data):
                    repairs['repaired'].append(wf_name)
                    repairs['changes_made'] = True
                else:
                    repairs['failed'].append(wf_name)

        # Save repair history
        self.repairs.append(repairs)
        self.save_json(self.repairs_db, self.repairs)

        return repairs

    def _repair_workflow(self, wf_name: str, wf_data: Dict) -> bool:
        """Repair individual workflow"""
        try:
            wf_path = self.workflow_path / wf_name

            if not wf_path.exists():
                # Create missing workflow
                return self._create_missing_workflow(wf_name)

            # Load workflow
            with open(wf_path, 'r') as f:
                workflow = yaml.safe_load(f) or {}

            modified = False

            # Fix common issues
            for issue in wf_data.get('issues', []):
                if 'No jobs defined' in issue:
                    workflow['jobs'] = self._get_default_job()
                    modified = True

                elif 'Invalid cron' in issue:
                    # Fix cron expression
                    if 'on' in workflow and 'schedule' in workflow['on']:
                        workflow['on']['schedule'] = [{'cron': '0 */6 * * *'}]  # Default to every 6 hours
                        modified = True

                elif 'No error handling' in issue:
                    # Add error handling
                    for job_name, job in workflow.get('jobs', {}).items():
                        if 'critical' in job_name.lower():
                            for step in job.get('steps', []):
                                if isinstance(step, dict) and 'continue-on-error' not in step:
                                    step['continue-on-error'] = True
                            modified = True

                elif 'timeout' in issue.lower():
                    # Set appropriate timeout
                    for job_name, job in workflow.get('jobs', {}).items():
                        if 'timeout-minutes' not in job:
                            job['timeout-minutes'] = 30
                            modified = True

            # Apply optimizations
            for optimization in wf_data.get('optimizations', []):
                if 'Add caching' in optimization:
                    # Add caching to Python setup
                    for job in workflow.get('jobs', {}).values():
                        for step in job.get('steps', []):
                            if isinstance(step, dict) and 'setup-python' in step.get('uses', ''):
                                if 'with' not in step:
                                    step['with'] = {}
                                step['with']['cache'] = 'pip'
                                modified = True

                elif 'branch filter' in optimization:
                    # Add branch filters
                    if 'on' in workflow:
                        if 'push' in workflow['on'] and not isinstance(workflow['on']['push'], dict):
                            workflow['on']['push'] = {'branches': ['main']}
                            modified = True
                        if 'pull_request' in workflow['on'] and not isinstance(workflow['on']['pull_request'], dict):
                            workflow['on']['pull_request'] = {'branches': ['main']}
                            modified = True

            if modified:
                # Save repaired workflow
                with open(wf_path, 'w') as f:
                    yaml.dump(workflow, f, default_flow_style=False, sort_keys=False)

                print(f"    ‚úÖ Repaired {wf_name}")
                return True

            return False

        except Exception as e:
            print(f"    ‚ùå Failed to repair {wf_name}: {e}")
            return False

    def _create_missing_workflow(self, wf_name: str) -> bool:
        """Create a missing workflow file"""
        try:
            # Determine workflow type from name
            if 'ml' in wf_name.lower() or 'train' in wf_name.lower():
                template = self._get_ml_workflow_template(wf_name)
            elif 'trade' in wf_name.lower() or 'trading' in wf_name.lower():
                template = self._get_trading_workflow_template(wf_name)
            elif 'data' in wf_name.lower() or 'collect' in wf_name.lower():
                template = self._get_data_workflow_template(wf_name)
            else:
                template = self._get_generic_workflow_template(wf_name)

            # Save workflow
            wf_path = self.workflow_path / wf_name
            wf_path.write_text(template)

            print(f"    ‚úÖ Created missing workflow: {wf_name}")
            return True

        except Exception as e:
            print(f"    ‚ùå Failed to create {wf_name}: {e}")
            return False

    def _get_default_job(self) -> Dict:
        """Get default job configuration"""
        return {
            'default-job': {
                'runs-on': 'ubuntu-latest',
                'timeout-minutes': 30,
                'steps': [
                    {'uses': 'actions/checkout@v4'},
                    {'uses': 'actions/setup-python@v4', 'with': {'python-version': '3.10', 'cache': 'pip'}},
                    {'name': 'Run script', 'run': 'echo "Placeholder job"'}
                ]
            }
        }

    # ========================================
    # WORKFLOW OPTIMIZATION
    # ========================================

    def optimize_workflows(self) -> Dict:
        """Optimize all workflows for speed and efficiency"""
        print("\n‚ö° OPTIMIZING WORKFLOWS...")

        optimizations = {
            'timestamp': datetime.utcnow().isoformat(),
            'optimized': [],
            'minutes_saved': 0,
            'changes_made': False
        }

        for wf_name in self.workflows:
            wf_path = self.workflow_path / wf_name

            if wf_path.exists():
                saved = self._optimize_workflow(wf_path)
                if saved > 0:
                    optimizations['optimized'].append(wf_name)
                    optimizations['minutes_saved'] += saved
                    optimizations['changes_made'] = True

        return optimizations

    def _optimize_workflow(self, wf_path: Path) -> int:
        """Optimize individual workflow, returns minutes saved"""
        try:
            with open(wf_path, 'r') as f:
                workflow = yaml.safe_load(f)

            if not workflow:
                return 0

            original_workflow = yaml.dump(workflow)
            minutes_saved = 0

            # Optimization 1: Add caching
            for job_name, job in workflow.get('jobs', {}).items():
                # Add dependency caching
                has_cache = False
                for step in job.get('steps', []):
                    if isinstance(step, dict):
                        # Add cache to setup-python
                        if 'setup-python' in step.get('uses', ''):
                            if 'with' not in step:
                                step['with'] = {}
                            if 'cache' not in step['with']:
                                step['with']['cache'] = 'pip'
                                minutes_saved += 2

                        # Check for existing cache
                        if 'cache' in step.get('uses', ''):
                            has_cache = True

                # Add cache action if not present and there are many steps
                if not has_cache and len(job.get('steps', [])) > 5:
                    cache_step = {
                        'name': 'Cache dependencies',
                        'uses': 'actions/cache@v3',
                        'with': {
                            'path': '~/.cache/pip',
                            'key': "${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}",
                            'restore-keys': "${{ runner.os }}-pip-"
                        }
                    }
                    job['steps'].insert(2, cache_step)
                    minutes_saved += 3

            # Optimization 2: Shallow clone for faster checkout
            for job in workflow.get('jobs', {}).values():
                for step in job.get('steps', []):
                    if isinstance(step, dict) and 'checkout' in step.get('uses', ''):
                        if 'with' not in step:
                            step['with'] = {}
                        if 'fetch-depth' not in step['with']:
                            step['with']['fetch-depth'] = 1  # Shallow clone
                            minutes_saved += 1

            # Optimization 3: Set appropriate timeouts
            for job_name, job in workflow.get('jobs', {}).items():
                if 'timeout-minutes' not in job:
                    # Estimate based on steps
                    estimated_time = len(job.get('steps', [])) * 3
                    job['timeout-minutes'] = min(estimated_time, 30)
                elif job['timeout-minutes'] > 60:
                    job['timeout-minutes'] = 45  # Reduce excessive timeouts
                    minutes_saved += 5

            # Optimization 4: Conditional execution
            if 'on' in workflow and 'push' in workflow['on']:
                if not isinstance(workflow['on']['push'], dict):
                    workflow['on']['push'] = {
                        'branches': ['main'],
                        'paths-ignore': ['**.md', 'docs/**', '.gitignore']
                    }
                    minutes_saved += 10  # Avoid unnecessary runs

            # Save if modified
            new_workflow = yaml.dump(workflow)
            if new_workflow != original_workflow:
                with open(wf_path, 'w') as f:
                    yaml.dump(workflow, f, default_flow_style=False, sort_keys=False)

                print(f"    ‚ö° Optimized {wf_path.name} (saved ~{minutes_saved} min/month)")

            return minutes_saved

        except Exception as e:
            print(f"    ‚ùå Failed to optimize {wf_path.name}: {e}")
            return 0

    # ========================================
    # REPORTING & ALERTS
    # ========================================

    def generate_report(self) -> Dict:
        """Generate comprehensive report"""
        report = {
            'timestamp': datetime.utcnow().isoformat(),
            'version': self.version,
            'repository': f"{self.repo_owner}/{self.repo_name}",
            'summary': {
                'total_workflows': len(self.workflows),
                'healthy_workflows': 0,
                'issues_found': 0,
                'repairs_made': 0,
                'optimizations_applied': 0,
                'estimated_monthly_minutes': 0,
                'minutes_saved': 0
            },
            'details': {
                'workflows': self.workflows,
                'recent_repairs': self.repairs[-5:] if self.repairs else [],
                'critical_issues': [],
                'recommendations': []
            }
        }

        # Calculate summary statistics
        for wf_data in self.workflows.values():
            if not wf_data.get('issues'):
                report['summary']['healthy_workflows'] += 1
            report['summary']['issues_found'] += len(wf_data.get('issues', []))
            report['summary']['estimated_monthly_minutes'] += (
                wf_data.get('runs_per_month', 0) * wf_data.get('estimated_minutes', 5)
            )

        # Add critical issues
        if report['summary']['estimated_monthly_minutes'] > 20000:
            report['details']['critical_issues'].append(
                f"CRITICAL: Minute usage ({report['summary']['estimated_monthly_minutes']}) exceeds limit!"
            )

        # Add recommendations
        if report['summary']['estimated_monthly_minutes'] > 15000:
            report['details']['recommendations'].extend([
                "Implement aggressive caching across all workflows",
                "Reduce workflow frequency for non-critical tasks",
                "Use matrix strategies to combine similar jobs",
                "Consider self-hosted runners for heavy workloads"
            ])

        # Save report
        report_path = self.cloud_path / 'reports' / f'report_{datetime.utcnow().strftime("%Y%m%d_%H%M%S")}.json'
        self.save_json(report_path, report)

        # Also save as latest
        self.save_json(self.cloud_path / 'reports' / 'latest_report.json', report)

        return report

    def generate_alerts(self, analysis: Dict):
        """Generate alerts for critical issues"""
        alerts = []

        # Check for critical issues
        if analysis['total_monthly_minutes'] > 20000:
            alerts.append({
                'level': 'CRITICAL',
                'title': 'Minute Usage Exceeds Limit!',
                'message': f"Using {analysis['total_monthly_minutes']}/20000 minutes",
                'action': 'Immediate optimization required'
            })

        if len(analysis['broken_workflows']) > 0:
            alerts.append({
                'level': 'HIGH',
                'title': f"{len(analysis['broken_workflows'])} Broken Workflows",
                'workflows': analysis['broken_workflows'],
                'action': 'Repair required'
            })

        if analysis['schedule_conflicts']:
            alerts.append({
                'level': 'MEDIUM',
                'title': 'Schedule Conflicts Detected',
                'conflicts': analysis['schedule_conflicts'],
                'action': 'Reschedule workflows to avoid conflicts'
            })

        # Save alerts
        if alerts:
            # Save critical alert for GitHub issue creation
            critical_alerts = [a for a in alerts if a['level'] == 'CRITICAL']
            if critical_alerts:
                alert_data = {
                    'title': critical_alerts[0]['title'],
                    'body': self._format_alert_body(alerts),
                    'timestamp': datetime.utcnow().isoformat()
                }
                self.save_json(self.cloud_path / 'alerts' / 'critical.json', alert_data)

            # Save all alerts
            self.save_json(self.cloud_path / 'alerts' / 'all_alerts.json', alerts)

        return alerts

    def _format_alert_body(self, alerts: List[Dict]) -> str:
        """Format alerts for GitHub issue"""
        body = "# üö® Cloud Bot Mechanic Alert Report\n\n"
        body += f"**Generated:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}\n"
        body += f"**Repository:** {self.repo_owner}/{self.repo_name}\n\n"

        for alert in alerts:
            level_emoji = {
                'CRITICAL': 'üî¥',
                'HIGH': 'üü†',
                'MEDIUM': 'üü°',
                'LOW': 'üü¢'
            }.get(alert['level'], '‚ö™')

            body += f"## {level_emoji} {alert['level']}: {alert['title']}\n\n"

            if 'message' in alert:
                body += f"{alert['message']}\n\n"

            if 'workflows' in alert:
                body += "**Affected Workflows:**\n"
                for wf in alert['workflows']:
                    body += f"- {wf}\n"
                body += "\n"

            if 'action' in alert:
                body += f"**Recommended Action:** {alert['action']}\n\n"

            body += "---\n\n"

        body += "*This alert was automatically generated by Cloud Bot Mechanic v3.0*"
        return body

    def _print_analysis_report(self, analysis: Dict):
        """Print analysis report to console"""
        print("\n" + "="*60)
        print("üìä WORKFLOW ANALYSIS REPORT")
        print("="*60)
        print(f"Total workflows: {analysis['total_workflows']}")
        print(f"Healthy workflows: {analysis['healthy_workflows']}")
        print(f"Issues found: {len(analysis['issues_found'])}")
        print(f"Estimated monthly minutes: {analysis['total_monthly_minutes']:,}")

        if analysis['broken_workflows']:
            print(f"\n‚ùå Broken Workflows ({len(analysis['broken_workflows'])}):")
            for wf in analysis['broken_workflows'][:5]:
                print(f"  ‚Ä¢ {wf}")

        if analysis['slow_workflows']:
            print(f"\nüêå Slow Workflows ({len(analysis['slow_workflows'])}):")
            for wf in analysis['slow_workflows'][:5]:
                print(f"  ‚Ä¢ {wf}")

        if analysis['expensive_workflows']:
            print(f"\nüí∞ Expensive Workflows ({len(analysis['expensive_workflows'])}):")
            for exp in analysis['expensive_workflows'][:5]:
                print(f"  ‚Ä¢ {exp['workflow']}: {exp['monthly_minutes']:,} min/month")

        if analysis['schedule_conflicts']:
            print(f"\n‚ö†Ô∏è Schedule Conflicts:")
            for conflict in analysis['schedule_conflicts'][:3]:
                print(f"  ‚Ä¢ {conflict['hour']}: {conflict['count']} workflows")

        print("="*60)

    # ========================================
    # WORKFLOW TEMPLATES
    # ========================================

    def _get_ml_workflow_template(self, name: str) -> str:
        """Get ML workflow template"""
        return f"""name: {name.replace('.yml', '').replace('_', ' ').title()}

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours
  workflow_dispatch:

jobs:
  train-models:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{{{ runner.os }}}}-pip-${{{{ hashFiles('**/requirements.txt') }}}}
        restore-keys: ${{{{ runner.os }}}}-pip-

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install pandas numpy scikit-learn xgboost lightgbm
      continue-on-error: true

    - name: Train models
      run: |
        python Intelligence/scripts/ml/train_models.py || echo "Training failed, using backup"
      continue-on-error: true

    - name: Save models
      uses: actions/upload-artifact@v3
      with:
        name: trained-models
        path: Intelligence/models/
        retention-days: 7
"""

    def _get_trading_workflow_template(self, name: str) -> str:
        """Get trading workflow template"""
        return f"""name: {name.replace('.yml', '').replace('_', ' ').title()}

on:
  schedule:
    - cron: '*/15 9-16 * * 1-5'  # Every 15 min during market hours
  workflow_dispatch:

jobs:
  trading-signals:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install pandas numpy yfinance requests ta
      continue-on-error: true

    - name: Generate signals
      run: |
        python Intelligence/scripts/strategies/generate_signals.py || echo "Using default signals"
      continue-on-error: true

    - name: Save signals
      uses: actions/upload-artifact@v3
      with:
        name: trading-signals
        path: Intelligence/data/*signals*.json
        retention-days: 1
"""

    def _get_data_workflow_template(self, name: str) -> str:
        """Get data collection workflow template"""
        return f"""name: {name.replace('.yml', '').replace('_', ' ').title()}

on:
  schedule:
    - cron: '0 */3 * * *'  # Every 3 hours
  workflow_dispatch:

jobs:
  collect-data:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install pandas yfinance requests
      continue-on-error: true

    - name: Collect data
      run: |
        python Intelligence/scripts/data/collect_all.py || echo "Data collection failed"
      continue-on-error: true

    - name: Save data
      uses: actions/upload-artifact@v3
      with:
        name: market-data
        path: Intelligence/data/
        retention-days: 3
"""

    def _get_generic_workflow_template(self, name: str) -> str:
        """Get generic workflow template"""
        return f"""name: {name.replace('.yml', '').replace('_', ' ').title()}

on:
  schedule:
    - cron: '0 */12 * * *'  # Twice daily
  workflow_dispatch:

jobs:
  main-job:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt || pip install pandas numpy
      continue-on-error: true

    - name: Run script
      run: |
        echo "Running {name}"
        # Add your script here
      continue-on-error: true
"""

    # ========================================
    # MAIN EXECUTION
    # ========================================

    def run(self) -> Dict:
        """Main execution flow"""
        print("\n" + "="*70)
        print("üå©Ô∏è CLOUD BOT MECHANIC - STARTING")
        print("="*70)

        results = {
            'start_time': self.start_time.isoformat(),
            'mode': self.operation_mode,
            'outputs': {}
        }

        # Step 1: Analyze all workflows
        analysis = self.analyze_all_workflows()
        results['analysis'] = analysis

        # Step 2: Repair broken workflows
        if analysis['broken_workflows'] or analysis['issues_found']:
            repairs = self.repair_broken_workflows()
            results['repairs'] = repairs

            # Set output for GitHub Actions
            print(f"::set-output name=repairs_needed::true")
            print(f"::set-output name=changes_made::{repairs['changes_made']}")
        else:
            print(f"::set-output name=repairs_needed::false")

        # Step 3: Optimize workflows if needed
        if analysis['total_monthly_minutes'] > 15000 or analysis['optimization_opportunities']:
            optimizations = self.optimize_workflows()
            results['optimizations'] = optimizations

            print(f"::set-output name=needs_optimization::true")
            if optimizations['changes_made']:
                print(f"::set-output name=changes_made::true")
        else:
            print(f"::set-output name=needs_optimization::false")

        # Step 4: Generate report
        report = self.generate_report()
        results['report'] = report

        # Step 5: Generate alerts
        alerts = self.generate_alerts(analysis)
        results['alerts'] = alerts

        # Step 6: Create commit message
        if results.get('repairs', {}).get('changes_made') or results.get('optimizations', {}).get('changes_made'):
            commit_msg = self._generate_commit_message(results)
            commit_msg_path = self.cloud_path / 'logs' / 'commit_message.txt'
            commit_msg_path.write_text(commit_msg)
            print(f"::set-output name=changes_made::true")

        # Save complete results
        results_path = self.cloud_path / 'database' / 'latest_run.json'
        self.save_json(results_path, results)

        print("\n" + "="*70)
        print("üå©Ô∏è CLOUD BOT MECHANIC - COMPLETE")
        print("="*70)
        print(f"Analyzed: {analysis['total_workflows']} workflows")
        print(f"Issues found: {len(analysis['issues_found'])}")
        print(f"Repairs made: {len(results.get('repairs', {}).get('repaired', []))}")
        print(f"Optimizations: {len(results.get('optimizations', {}).get('optimized', []))}")
        print(f"Monthly minutes: {analysis['total_monthly_minutes']:,}/20,000")
        print("="*70)

        return results

    def _generate_commit_message(self, results: Dict) -> str:
        """Generate detailed commit message"""
        msg = "üå©Ô∏è Cloud Mechanic: Auto-repairs and optimizations\n\n"

        if results.get('repairs', {}).get('repaired'):
            msg += "Repairs:\n"
            for wf in results['repairs']['repaired']:
                msg += f"- Fixed {wf}\n"
            msg += "\n"

        if results.get('optimizations', {}).get('optimized'):
            msg += "Optimizations:\n"
            for wf in results['optimizations']['optimized']:
                msg += f"- Optimized {wf}\n"
            msg += f"\nEstimated minutes saved: {results['optimizations']['minutes_saved']}/month\n"

        msg += f"\nWorkflow health: {results['report']['summary']['healthy_workflows']}/{results['report']['summary']['total_workflows']}"

        return msg


# Main execution
if __name__ == "__main__":
    mechanic = CloudBotMechanic()
    mechanic.run()
EOF

# ============================================
# WORKFLOW ANALYZER MODULE
# ============================================

cat > Intelligence/mechanic/cloud/workflow_analyzer.py << 'EOF'
#!/usr/bin/env python3
"""
WORKFLOW ANALYZER - Analyzes workflow health and performance
"""

import os
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

class WorkflowAnalyzer:
    def __init__(self):
        self.base_path = Path.cwd()
        self.workflow_path = Path('.github/workflows')
        self.cloud_path = Path('Intelligence/mechanic/cloud')
        self.db_path = self.cloud_path / 'database'

        # Load existing data
        self.workflows = self.load_json(self.db_path / 'workflows.json', {})

    def load_json(self, path: Path, default: Any) -> Any:
        try:
            if path.exists():
                with open(path, 'r') as f:
                    return json.load(f)
        except:
            pass
        return default

    def analyze_health(self) -> Dict:
        """Analyze overall workflow health"""
        print("üîç Analyzing workflow health...")

        health_report = {
            'timestamp': datetime.utcnow().isoformat(),
            'total_workflows': len(self.workflows),
            'healthy_count': 0,
            'issues_count': 0,
            'critical_issues': [],
            'performance_metrics': {},
            'recommendations': []
        }

        for wf_name, wf_data in self.workflows.items():
            issues = wf_data.get('issues', [])
            if not issues:
                health_report['healthy_count'] += 1
            else:
                health_report['issues_count'] += 1

                # Check for critical issues
                for issue in issues:
                    if any(keyword in issue.lower() for keyword in ['critical', 'broken', 'failed']):
                        health_report['critical_issues'].append({
                            'workflow': wf_name,
                            'issue': issue
                        })

        # Generate recommendations
        if health_report['issues_count'] > 0:
            health_report['recommendations'].append(
                f"Fix {health_report['issues_count']} workflow issues"
            )

        if health_report['healthy_count'] < len(self.workflows) * 0.8:
            health_report['recommendations'].append(
                "Improve overall workflow health (>80% should be healthy)"
            )

        # Save health report
        self.save_json(self.cloud_path / 'reports' / 'health_analysis.json', health_report)

        print(f"‚úÖ Health analysis complete: {health_report['healthy_count']}/{health_report['total_workflows']} healthy")

        return health_report

    def save_json(self, path: Path, data: Any):
        """Save JSON data"""
        with open(path, 'w') as f:
            json.dump(data, f, indent=2, default=str)

if __name__ == "__main__":
    analyzer = WorkflowAnalyzer()
    analyzer.analyze_health()
EOF

# ============================================
# WORKFLOW OPTIMIZER MODULE
# ============================================

cat > Intelligence/mechanic/cloud/workflow_optimizer.py << 'EOF'
#!/usr/bin/env python3
"""
WORKFLOW OPTIMIZER - Optimizes workflows for speed and efficiency
"""

import os
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

class WorkflowOptimizer:
    def __init__(self):
        self.base_path = Path.cwd()
        self.workflow_path = Path('.github/workflows')
        self.cloud_path = Path('Intelligence/mechanic/cloud')
        self.db_path = self.cloud_path / 'database'

        # Load existing data
        self.workflows = self.load_json(self.db_path / 'workflows.json', {})

    def load_json(self, path: Path, default: Any) -> Any:
        try:
            if path.exists():
                with open(path, 'r') as f:
                    return json.load(f)
        except:
            pass
        return default

    def optimize_all(self) -> Dict:
        """Optimize all workflows"""
        print("‚ö° Optimizing workflows...")

        optimization_report = {
            'timestamp': datetime.utcnow().isoformat(),
            'optimized_workflows': [],
            'total_minutes_saved': 0,
            'optimizations_applied': [],
            'failed_optimizations': []
        }

        for wf_name, wf_data in self.workflows.items():
            wf_path = self.workflow_path / wf_name

            if wf_path.exists():
                try:
                    result = self.optimize_workflow(wf_path, wf_data)
                    if result['optimized']:
                        optimization_report['optimized_workflows'].append(wf_name)
                        optimization_report['total_minutes_saved'] += result['minutes_saved']
                        optimization_report['optimizations_applied'].extend(result['applied'])
                except Exception as e:
                    optimization_report['failed_optimizations'].append({
                        'workflow': wf_name,
                        'error': str(e)
                    })

        # Save optimization report
        self.save_json(self.cloud_path / 'reports' / 'optimization_report.json', optimization_report)

        print(f"‚úÖ Optimization complete: {len(optimization_report['optimized_workflows'])} workflows optimized")
        print(f"üí∞ Estimated minutes saved: {optimization_report['total_minutes_saved']}/month")

        return optimization_report

    def optimize_workflow(self, wf_path: Path, wf_data: Dict) -> Dict:
        """Optimize individual workflow"""
        result = {
            'optimized': False,
            'minutes_saved': 0,
            'applied': []
        }

        try:
            with open(wf_path, 'r') as f:
                workflow = yaml.safe_load(f)

            if not workflow:
                return result

            original_content = yaml.dump(workflow)
            optimizations_applied = []

            # Optimization 1: Add caching
            for job_name, job in workflow.get('jobs', {}).items():
                if self.add_caching(job, job_name):
                    optimizations_applied.append(f"Added caching to {job_name}")
                    result['minutes_saved'] += 2

            # Optimization 2: Set timeouts
            for job_name, job in workflow.get('jobs', {}).items():
                if self.optimize_timeout(job, job_name):
                    optimizations_applied.append(f"Optimized timeout for {job_name}")
                    result['minutes_saved'] += 1

            # Optimization 3: Shallow clone
            for job in workflow.get('jobs', {}).values():
                if self.add_shallow_clone(job):
                    optimizations_applied.append("Added shallow clone")
                    result['minutes_saved'] += 1

            # Save if optimized
            if optimizations_applied:
                with open(wf_path, 'w') as f:
                    yaml.dump(workflow, f, default_flow_style=False, sort_keys=False)

                result['optimized'] = True
                result['applied'] = optimizations_applied

        except Exception as e:
            print(f"‚ùå Failed to optimize {wf_path.name}: {e}")

        return result

    def add_caching(self, job: Dict, job_name: str) -> bool:
        """Add caching to job"""
        if 'steps' not in job:
            return False

        has_cache = False
        has_python = False

        for step in job['steps']:
            if isinstance(step, dict):
                if 'cache' in step.get('uses', ''):
                    has_cache = True
                if 'setup-python' in step.get('uses', ''):
                    has_python = True

        if has_python and not has_cache:
            # Add cache step
            cache_step = {
                'name': 'Cache dependencies',
                'uses': 'actions/cache@v3',
                'with': {
                    'path': '~/.cache/pip',
                    'key': "${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}",
                    'restore-keys': "${{ runner.os }}-pip-"
                }
            }
            job['steps'].insert(2, cache_step)
            return True

        return False

    def optimize_timeout(self, job: Dict, job_name: str) -> bool:
        """Optimize job timeout"""
        if 'timeout-minutes' not in job:
            # Set reasonable timeout based on steps
            steps_count = len(job.get('steps', []))
            timeout = min(steps_count * 3, 30)
            job['timeout-minutes'] = timeout
            return True
        elif job['timeout-minutes'] > 60:
            job['timeout-minutes'] = 45
            return True

        return False

    def add_shallow_clone(self, job: Dict) -> bool:
        """Add shallow clone to checkout steps"""
        if 'steps' not in job:
            return False

        for step in job['steps']:
            if isinstance(step, dict) and 'checkout' in step.get('uses', ''):
                if 'with' not in step:
                    step['with'] = {}
                if 'fetch-depth' not in step['with']:
                    step['with']['fetch-depth'] = 1
                    return True

        return False

    def save_json(self, path: Path, data: Any):
        """Save JSON data"""
        with open(path, 'w') as f:
            json.dump(data, f, indent=2, default=str)

if __name__ == "__main__":
    optimizer = WorkflowOptimizer()
    optimizer.optimize_all()
EOF

# ============================================
# REPAIR SYSTEM MODULE
# ============================================

cat > Intelligence/mechanic/cloud/repair_system.py << 'EOF'
#!/usr/bin/env python3
"""
REPAIR SYSTEM - Automatically repairs broken workflows
"""

import os
import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

class RepairSystem:
    def __init__(self):
        self.base_path = Path.cwd()
        self.workflow_path = Path('.github/workflows')
        self.cloud_path = Path('Intelligence/mechanic/cloud')
        self.db_path = self.cloud_path / 'database'

        # Load existing data
        self.workflows = self.load_json(self.db_path / 'workflows.json', {})

    def load_json(self, path: Path, default: Any) -> Any:
        try:
            if path.exists():
                with open(path, 'r') as f:
                    return json.load(f)
        except:
            pass
        return default

    def repair_all(self) -> Dict:
        """Repair all broken workflows"""
        print("üîß Starting repair system...")

        repair_report = {
            'timestamp': datetime.utcnow().isoformat(),
            'repaired_workflows': [],
            'failed_repairs': [],
            'repairs_applied': [],
            'created_workflows': []
        }

        for wf_name, wf_data in self.workflows.items():
            issues = wf_data.get('issues', [])

            if issues:
                print(f"  Repairing {wf_name}...")
                result = self.repair_workflow(wf_name, wf_data)

                if result['success']:
                    repair_report['repaired_workflows'].append(wf_name)
                    repair_report['repairs_applied'].extend(result['repairs'])
                else:
                    repair_report['failed_repairs'].append({
                        'workflow': wf_name,
                        'error': result.get('error', 'Unknown error')
                    })

        # Create missing critical workflows
        critical_workflows = [
            'ultimate_ml_rl_intel_system.yml',
            'es_nq_critical_trading.yml',
            'options_flow_analysis.yml'
        ]

        for wf_name in critical_workflows:
            wf_path = self.workflow_path / wf_name
            if not wf_path.exists():
                if self.create_critical_workflow(wf_name):
                    repair_report['created_workflows'].append(wf_name)

        # Save repair report
        self.save_json(self.cloud_path / 'reports' / 'repair_report.json', repair_report)

        print(f"‚úÖ Repair complete: {len(repair_report['repaired_workflows'])} repaired, {len(repair_report['created_workflows'])} created")

        return repair_report

    def repair_workflow(self, wf_name: str, wf_data: Dict) -> Dict:
        """Repair individual workflow"""
        result = {
            'success': False,
            'repairs': [],
            'error': None
        }

        try:
            wf_path = self.workflow_path / wf_name

            if not wf_path.exists():
                # Create missing workflow
                if self.create_missing_workflow(wf_name):
                    result['success'] = True
                    result['repairs'].append('Created missing workflow')
                return result

            # Load and repair existing workflow
            with open(wf_path, 'r') as f:
                workflow = yaml.safe_load(f) or {}

            original_content = yaml.dump(workflow)
            repairs_applied = []

            # Apply fixes based on issues
            issues = wf_data.get('issues', [])

            for issue in issues:
                if 'No jobs defined' in issue:
                    workflow['jobs'] = self.get_default_jobs()
                    repairs_applied.append('Added default jobs')

                elif 'Invalid cron' in issue:
                    if 'on' in workflow and 'schedule' in workflow['on']:
                        workflow['on']['schedule'] = [{'cron': '0 */6 * * *'}]
                        repairs_applied.append('Fixed invalid cron expression')

                elif 'No error handling' in issue:
                    for job in workflow.get('jobs', {}).values():
                        for step in job.get('steps', []):
                            if isinstance(step, dict) and 'continue-on-error' not in step:
                                step['continue-on-error'] = True
                    repairs_applied.append('Added error handling')

                elif 'timeout' in issue.lower():
                    for job in workflow.get('jobs', {}).values():
                        if 'timeout-minutes' not in job:
                            job['timeout-minutes'] = 30
                    repairs_applied.append('Set explicit timeout')

                elif 'branch filter' in issue:
                    if 'on' in workflow:
                        for trigger in ['push', 'pull_request']:
                            if trigger in workflow['on']:
                                if not isinstance(workflow['on'][trigger], dict):
                                    workflow['on'][trigger] = {'branches': ['main']}
                    repairs_applied.append('Added branch filters')

            # Save if repaired
            if repairs_applied:
                with open(wf_path, 'w') as f:
                    yaml.dump(workflow, f, default_flow_style=False, sort_keys=False)

                result['success'] = True
                result['repairs'] = repairs_applied

        except Exception as e:
            result['error'] = str(e)
            print(f"‚ùå Failed to repair {wf_name}: {e}")

        return result

    def create_missing_workflow(self, wf_name: str) -> bool:
        """Create a missing workflow file"""
        try:
            # Determine workflow type and create appropriate template
            if 'ml' in wf_name.lower() or 'train' in wf_name.lower():
                template = self.get_ml_template(wf_name)
            elif 'trade' in wf_name.lower() or 'trading' in wf_name.lower():
                template = self.get_trading_template(wf_name)
            elif 'data' in wf_name.lower() or 'collect' in wf_name.lower():
                template = self.get_data_template(wf_name)
            else:
                template = self.get_generic_template(wf_name)

            # Save workflow
            wf_path = self.workflow_path / wf_name
            wf_path.write_text(template)

            print(f"    ‚úÖ Created missing workflow: {wf_name}")
            return True

        except Exception as e:
            print(f"    ‚ùå Failed to create {wf_name}: {e}")
            return False

    def create_critical_workflow(self, wf_name: str) -> bool:
        """Create a critical workflow that must exist"""
        try:
            template = f"""name: {wf_name.replace('.yml', '').replace('_', ' ').title()}

on:
  schedule:
    - cron: '*/30 * * * *'  # Every 30 minutes
  workflow_dispatch:

jobs:
  critical-job:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install pandas numpy requests
      continue-on-error: true

    - name: Run critical task
      run: |
        echo "Running critical workflow: {wf_name}"
        # Add your critical logic here
      continue-on-error: true
"""

            wf_path = self.workflow_path / wf_name
            wf_path.write_text(template)

            print(f"    ‚úÖ Created critical workflow: {wf_name}")
            return True

        except Exception as e:
            print(f"    ‚ùå Failed to create critical workflow {wf_name}: {e}")
            return False

    def get_default_jobs(self) -> Dict:
        """Get default job configuration"""
        return {
            'default-job': {
                'runs-on': 'ubuntu-latest',
                'timeout-minutes': 30,
                'steps': [
                    {'uses': 'actions/checkout@v4'},
                    {'uses': 'actions/setup-python@v4', 'with': {'python-version': '3.10', 'cache': 'pip'}},
                    {'name': 'Run script', 'run': 'echo "Default job executed"'}
                ]
            }
        }

    def get_ml_template(self, name: str) -> str:
        """Get ML workflow template"""
        return f"""name: {name.replace('.yml', '').replace('_', ' ').title()}

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  train-models:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install pandas numpy scikit-learn
      continue-on-error: true

    - name: Train models
      run: |
        python Intelligence/scripts/ml/train_models.py || echo "Training completed"
      continue-on-error: true
"""

    def get_trading_template(self, name: str) -> str:
        """Get trading workflow template"""
        return f"""name: {name.replace('.yml', '').replace('_', ' ').title()}

on:
  schedule:
    - cron: '*/15 9-16 * * 1-5'
  workflow_dispatch:

jobs:
  trading-signals:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install pandas yfinance requests
      continue-on-error: true

    - name: Generate signals
      run: |
        python Intelligence/scripts/strategies/generate_signals.py || echo "Signals generated"
      continue-on-error: true
"""

    def get_data_template(self, name: str) -> str:
        """Get data collection workflow template"""
        return f"""name: {name.replace('.yml', '').replace('_', ' ').title()}

on:
  schedule:
    - cron: '0 */3 * * *'
  workflow_dispatch:

jobs:
  collect-data:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install pandas requests
      continue-on-error: true

    - name: Collect data
      run: |
        python Intelligence/scripts/data/collect_all.py || echo "Data collected"
      continue-on-error: true
"""

    def get_generic_template(self, name: str) -> str:
        """Get generic workflow template"""
        return f"""name: {name.replace('.yml', '').replace('_', ' ').title()}

on:
  schedule:
    - cron: '0 */12 * * *'
  workflow_dispatch:

jobs:
  main-job:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 1

    - uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'

    - name: Install dependencies
      run: |
        pip install pandas numpy
      continue-on-error: true

    - name: Run script
      run: |
        echo "Running {name}"
      continue-on-error: true
"""

    def save_json(self, path: Path, data: Any):
        """Save JSON data"""
        with open(path, 'w') as f:
            json.dump(data, f, indent=2, default=str)

if __name__ == "__main__":
    repair_system = RepairSystem()
    repair_system.repair_all()
EOF

# ============================================
# REPORT GENERATOR MODULE
# ============================================

cat > Intelligence/mechanic/cloud/report_generator.py << 'EOF'
#!/usr/bin/env python3
"""
REPORT GENERATOR - Generates comprehensive reports
"""

import os
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any

class ReportGenerator:
    def __init__(self):
        self.base_path = Path.cwd()
        self.cloud_path = Path('Intelligence/mechanic/cloud')
        self.reports_path = self.cloud_path / 'reports'
        self.db_path = self.cloud_path / 'database'

        # Ensure reports directory exists
        self.reports_path.mkdir(exist_ok=True)

    def generate_comprehensive_report(self) -> Dict:
        """Generate comprehensive report"""
        print("üìä Generating comprehensive report...")

        # Load all data
        workflows = self.load_json(self.db_path / 'workflows.json', {})
        performance = self.load_json(self.db_path / 'performance.json', {})
        issues = self.load_json(self.db_path / 'issues.json', [])
        repairs = self.load_json(self.db_path / 'repairs.json', [])

        # Load latest analysis
        latest_run = self.load_json(self.db_path / 'latest_run.json', {})

        # Generate report
        report = {
            'timestamp': datetime.utcnow().isoformat(),
            'version': '3.0-CLOUD',
            'summary': {
                'total_workflows': len(workflows),
                'healthy_workflows': 0,
                'issues_found': 0,
                'repairs_made': 0,
                'optimizations_applied': 0,
                'estimated_monthly_minutes': 0,
                'minutes_saved': 0
            },
            'details': {
                'workflows': workflows,
                'performance': performance,
                'issues': issues[-10:] if issues else [],  # Last 10 issues
                'repairs': repairs[-5:] if repairs else [],  # Last 5 repairs
                'critical_issues': [],
                'recommendations': []
            },
            'metrics': {
                'uptime_percentage': 0,
                'average_runtime': 0,
                'failure_rate': 0,
                'optimization_efficiency': 0
            }
        }

        # Calculate summary statistics
        for wf_data in workflows.values():
            if not wf_data.get('issues'):
                report['summary']['healthy_workflows'] += 1
            report['summary']['issues_found'] += len(wf_data.get('issues', []))
            report['summary']['estimated_monthly_minutes'] += (
                wf_data.get('runs_per_month', 0) * wf_data.get('estimated_minutes', 5)
            )

        # Add repair statistics
        for repair in repairs:
            report['summary']['repairs_made'] += len(repair.get('repaired', []))

        # Add optimization statistics
        if latest_run.get('optimizations'):
            report['summary']['optimizations_applied'] = len(
                latest_run['optimizations'].get('optimized', [])
            )
            report['summary']['minutes_saved'] = latest_run['optimizations'].get('minutes_saved', 0)

        # Generate critical issues
        if report['summary']['estimated_monthly_minutes'] > 20000:
            report['details']['critical_issues'].append(
                f"CRITICAL: Minute usage ({report['summary']['estimated_monthly_minutes']}) exceeds limit!"
            )

        if report['summary']['healthy_workflows'] < len(workflows) * 0.5:
            report['details']['critical_issues'].append(
                "CRITICAL: Less than 50% of workflows are healthy!"
            )

        # Generate recommendations
        if report['summary']['issues_found'] > 0:
            report['details']['recommendations'].append(
                f"Fix {report['summary']['issues_found']} workflow issues"
            )

        if report['summary']['estimated_monthly_minutes'] > 15000:
            report['details']['recommendations'].extend([
                "Implement aggressive caching across all workflows",
                "Reduce workflow frequency for non-critical tasks",
                "Use matrix strategies to combine similar jobs",
                "Consider self-hosted runners for heavy workloads"
            ])

        # Calculate metrics
        total_runs = 0
        successful_runs = 0
        total_runtime = 0

        for wf_data in workflows.values():
            runs = wf_data.get('runs', [])
            total_runs += len(runs)

            for run in runs:
                if run.get('conclusion') == 'success':
                    successful_runs += 1
                total_runtime += run.get('run_time_minutes', 0)

        if total_runs > 0:
            report['metrics']['uptime_percentage'] = (successful_runs / total_runs) * 100
            report['metrics']['failure_rate'] = ((total_runs - successful_runs) / total_runs) * 100
            report['metrics']['average_runtime'] = total_runtime / total_runs

        # Save report
        report_file = self.reports_path / f'report_{datetime.utcnow().strftime("%Y%m%d_%H%M%S")}.json'
        self.save_json(report_file, report)

        # Also save as latest
        self.save_json(self.reports_path / 'latest_report.json', report)

        # Generate summary text
        self.generate_summary_text(report)

        print(f"‚úÖ Report generated: {report_file.name}")

        return report

    def generate_summary_text(self, report: Dict):
        """Generate human-readable summary"""
        summary = f"""# üå©Ô∏è Cloud Bot Mechanic Report
**Generated:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M UTC')}

## üìä Summary
- **Total Workflows:** {report['summary']['total_workflows']}
- **Healthy Workflows:** {report['summary']['healthy_workflows']}
- **Issues Found:** {report['summary']['issues_found']}
- **Repairs Made:** {report['summary']['repairs_made']}
- **Optimizations Applied:** {report['summary']['optimizations_applied']}
- **Monthly Minutes:** {report['summary']['estimated_monthly_minutes']:,}/20,000
- **Minutes Saved:** {report['summary']['minutes_saved']}/month

## üìà Metrics
- **Uptime:** {report['metrics']['uptime_percentage']:.1f}%
- **Failure Rate:** {report['metrics']['failure_rate']:.1f}%
- **Avg Runtime:** {report['metrics']['average_runtime']:.1f} min

## üö® Critical Issues
"""

        for issue in report['details']['critical_issues']:
            summary += f"- {issue}\n"

        summary += "\n## üí° Recommendations\n"
        for rec in report['details']['recommendations']:
            summary += f"- {rec}\n"

        # Save summary
        summary_file = self.reports_path / f'summary_{datetime.utcnow().strftime("%Y%m%d_%H%M%S")}.md'
        summary_file.write_text(summary)

    def load_json(self, path: Path, default: Any) -> Any:
        """Load JSON with default fallback"""
        try:
            if path.exists():
                with open(path, 'r') as f:
                    return json.load(f)
        except:
            pass
        return default

    def save_json(self, path: Path, data: Any):
        """Save JSON data"""
        with open(path, 'w') as f:
            json.dump(data, f, indent=2, default=str)

if __name__ == "__main__":
    generator = ReportGenerator()
    generator.generate_comprehensive_report()
EOF

# ============================================
# INSTALLATION COMPLETE
# ============================================

echo ""
echo "================================================"
echo "‚úÖ CLOUD BOT MECHANIC - INSTALLATION COMPLETE"
echo "================================================"
echo ""
echo "üìÅ Files Created:"
echo "  ‚Ä¢ .github/workflows/cloud_bot_mechanic.yml"
echo "  ‚Ä¢ Intelligence/mechanic/cloud/cloud_mechanic_core.py"
echo "  ‚Ä¢ Intelligence/mechanic/cloud/workflow_analyzer.py"
echo "  ‚Ä¢ Intelligence/mechanic/cloud/workflow_optimizer.py"
echo "  ‚Ä¢ Intelligence/mechanic/cloud/repair_system.py"
echo "  ‚Ä¢ Intelligence/mechanic/cloud/report_generator.py"
echo ""
echo "üöÄ Next Steps:"
echo "  1. Push these files to your repository"
echo "  2. The mechanic will run automatically every 10 minutes"
echo "  3. Check the Actions tab for mechanic runs"
echo "  4. Review reports in Intelligence/mechanic/cloud/reports/"
echo ""
echo "üîß Features:"
echo "  ‚Ä¢ Monitors all workflows 24/7"
echo "  ‚Ä¢ Auto-repairs broken workflows"
echo "  ‚Ä¢ Optimizes for speed and cost"
echo "  ‚Ä¢ Generates detailed reports"
echo "  ‚Ä¢ Creates GitHub issues for critical problems"
echo ""
echo "‚öôÔ∏è Configuration:"
echo "  ‚Ä¢ Edit .github/workflows/cloud_bot_mechanic.yml for scheduling"
echo "  ‚Ä¢ Modify Python files for custom logic"
echo "  ‚Ä¢ Add GITHUB_TOKEN secret for API access"
echo ""
echo "================================================"
