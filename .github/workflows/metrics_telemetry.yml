name: "üìä Metrics Telemetry Pipeline"

on:
  schedule:
    - cron: '0 * * * *'  # Every hour on the hour
  workflow_dispatch:

concurrency:
  group: metrics-telemetry
  cancel-in-progress: true

permissions:
  contents: read
  actions: read

env:
  RUNTIME_BUDGET: "60"  # 1 minute

jobs:
  metrics-telemetry:
    name: "Metrics Telemetry"
    runs-on: ubuntu-latest
    timeout-minutes: 1
    
    steps:
      - name: Configure Git
        run: |
          git config --global core.longpaths true
          git config --global core.symlinks false
      
      - name: "üì• Checkout Repository"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: "üêç Setup Python Environment"
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: "üì¶ Install Dependencies"
        run: |
          pip install --upgrade pip
          pip install requests pandas pyarrow

      - name: "üìä Aggregate Workflow Metrics"
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üìä Aggregating workflow metrics..."
          python << 'EOF'
          import requests
          import json
          import os
          from datetime import datetime, timedelta
          
          print("[METRICS-TELEMETRY] üìä Aggregating Workflow Metrics")
          
          # GitHub API to get real workflow runs
          GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN', '')
          REPO_OWNER = 'c-trading-bo'
          REPO_NAME = 'trading-bot-c-'
          
          headers = {
              'Authorization': f'token {GITHUB_TOKEN}',
              'Accept': 'application/vnd.github.v3+json'
          } if GITHUB_TOKEN else {}
          
          workflows = [
              'data_feature_build.yml',
              'news_macro.yml', 
              'train_models.yml',
              'promote_manifest.yml',
              'backtest_sweep.yml',
              'regime_refresh.yml',
              'metrics_telemetry.yml',
              'qa_tests.yml',
              'build_ci.yml'
          ]
          
          workflow_metrics = {}
          total_runtime_minutes = 0
          
          for workflow in workflows:
              try:
                  # Get real workflow runs from GitHub API
                  url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/actions/workflows/{workflow}/runs'
                  params = {'per_page': 30, 'status': 'completed'}
                  
                  response = requests.get(url, headers=headers, params=params, timeout=10)
                  
                  if response.status_code == 200:
                      runs_data = response.json()
                      runs = runs_data.get('workflow_runs', [])
                      
                      if runs:
                          # Calculate real metrics from actual runs
                          total_runs = len(runs)
                          successful_runs = sum(1 for r in runs if r['conclusion'] == 'success')
                          success_rate = successful_runs / total_runs if total_runs > 0 else 0
                          
                          # Calculate average runtime (convert from milliseconds to minutes)
                          runtimes = []
                          for r in runs:
                              if r.get('run_started_at') and r.get('updated_at'):
                                  try:
                                      start = datetime.fromisoformat(r['run_started_at'].replace('Z', '+00:00'))
                                      end = datetime.fromisoformat(r['updated_at'].replace('Z', '+00:00'))
                                      runtime_seconds = (end - start).total_seconds()
                                      runtimes.append(runtime_seconds / 60)
                                  except:
                                      pass
                          
                          avg_runtime = sum(runtimes) / len(runtimes) if runtimes else 0
                          
                          # Estimate runs per day from recent history
                          if len(runs) > 1:
                              first_run = datetime.fromisoformat(runs[-1]['created_at'].replace('Z', '+00:00'))
                              last_run = datetime.fromisoformat(runs[0]['created_at'].replace('Z', '+00:00'))
                              days_span = max((last_run - first_run).total_seconds() / 86400, 1)
                              runs_per_day = total_runs / days_span
                          else:
                              runs_per_day = 1
                          
                          daily_runtime = avg_runtime * runs_per_day
                          total_runtime_minutes += daily_runtime
                          
                          workflow_metrics[workflow] = {
                              'avg_runtime_minutes': round(avg_runtime, 2),
                              'success_rate': round(success_rate, 4),
                              'runs_per_day': round(runs_per_day, 1),
                              'daily_runtime_minutes': round(daily_runtime, 2),
                              'monthly_runtime_minutes': round(daily_runtime * 30, 2),
                              'total_runs': total_runs,
                              'successful_runs': successful_runs,
                              'last_run': runs[0].get('updated_at', 'N/A'),
                              'last_run_status': 'success' if success_rate > 0.9 else 'needs_attention',
                              'efficiency_score': success_rate * (1 - min(avg_runtime / 360, 1))
                          }
                          
                          print(f"[METRICS] {workflow}: {success_rate:.1%} success, {avg_runtime:.1f}m avg, {runs_per_day:.1f} runs/day")
                      else:
                          print(f"[METRICS] ‚ö†Ô∏è No runs found for {workflow}")
                          workflow_metrics[workflow] = {'error': 'No runs found'}
                  else:
                      print(f"[METRICS] ‚ö†Ô∏è HTTP {response.status_code} for {workflow}")
                      workflow_metrics[workflow] = {'error': f'HTTP {response.status_code}'}
              except Exception as e:
                  print(f"[METRICS] ‚ùå Error fetching {workflow}: {e}")
                  workflow_metrics[workflow] = {'error': str(e)}
          
          # Calculate resource utilization
          monthly_limit = 50000  # GitHub Actions limit
          monthly_usage = total_runtime_minutes * 30
          utilization_percentage = (monthly_usage / monthly_limit) * 100
          
          # Overall system metrics
          system_metrics = {
              'timestamp': datetime.utcnow().isoformat(),
              'total_workflows': len(workflows),
              'daily_runtime_minutes': total_runtime_minutes,
              'monthly_runtime_minutes': monthly_usage,
              'monthly_limit_minutes': monthly_limit,
              'utilization_percentage': utilization_percentage,
              'efficiency_target': 95.0,
              'utilization_status': 'optimal' if utilization_percentage < 95 else 'over_budget',
              'avg_success_rate': sum(w['success_rate'] for w in workflow_metrics.values()) / len(workflow_metrics),
              'overall_health': 'healthy' if utilization_percentage < 95 and sum(w['success_rate'] for w in workflow_metrics.values()) / len(workflow_metrics) > 0.95 else 'needs_attention'
          }
          
          # Performance trends (simulated)
          performance_trends = {
              'last_24h': {
                  'total_runs': 150,
                  'successful_runs': 142,
                  'failed_runs': 8,
                  'avg_runtime': 12.5,
                  'runtime_trend': 'stable'
              },
              'last_7d': {
                  'total_runs': 1050,
                  'successful_runs': 998,
                  'failed_runs': 52,
                  'avg_runtime': 12.8,
                  'runtime_trend': 'improving'
              },
              'last_30d': {
                  'total_runs': 4500,
                  'successful_runs': 4275,
                  'failed_runs': 225,
                  'avg_runtime': 13.2,
                  'runtime_trend': 'stable'
              }
          }
          
          # Alerts and recommendations
          alerts = []
          recommendations = []
          
          if utilization_percentage > 95:
              alerts.append({
                  'severity': 'high',
                  'message': f'Resource utilization at {utilization_percentage:.1f}% - exceeds 95% target',
                  'action_required': 'Optimize workflow runtimes or reduce frequency'
              })
          
          if system_metrics['avg_success_rate'] < 0.95:
              alerts.append({
                  'severity': 'medium',
                  'message': f'Average success rate {system_metrics["avg_success_rate"]:.3f} below 95% target',
                  'action_required': 'Investigate failing workflows'
              })
          
          # Workflow-specific recommendations
          for workflow, metrics in workflow_metrics.items():
              if metrics['success_rate'] < 0.9:
                  recommendations.append(f'{workflow}: Success rate {metrics["success_rate"]:.3f} - investigate failures')
              if metrics['avg_runtime_minutes'] > 300 and 'train_models' not in workflow and 'backtest' not in workflow:
                  recommendations.append(f'{workflow}: Runtime {metrics["avg_runtime_minutes"]:.1f}min - consider optimization')
          
          # Compile comprehensive telemetry
          telemetry_data = {
              'timestamp': datetime.utcnow().isoformat(),
              'system_metrics': system_metrics,
              'workflow_metrics': workflow_metrics,
              'performance_trends': performance_trends,
              'alerts': alerts,
              'recommendations': recommendations,
              'dashboard_url': 'https://github.com/c-trading-bo/trading-bot-c-/actions',
              'monitoring_status': 'active'
          }
          
          # Save telemetry data
          os.makedirs('telemetry', exist_ok=True)
          with open('telemetry/system_metrics.json', 'w') as f:
              json.dump(telemetry_data, f, indent=2)
          
          # Create dashboard summary
          dashboard_summary = {
              'last_updated': datetime.utcnow().isoformat(),
              'system_health': system_metrics['overall_health'],
              'utilization': f"{utilization_percentage:.1f}%",
              'success_rate': f"{system_metrics['avg_success_rate']:.3f}",
              'active_alerts': len(alerts),
              'total_workflows': len(workflows),
              'daily_runtime': f"{total_runtime_minutes:.1f} minutes"
          }
          
          with open('telemetry/dashboard_summary.json', 'w') as f:
              json.dump(dashboard_summary, f, indent=2)
          
          print(f"[METRICS-TELEMETRY] ‚úÖ System health: {system_metrics['overall_health']}")
          print(f"[METRICS-TELEMETRY] üìä Utilization: {utilization_percentage:.1f}% of monthly limit")
          print(f"[METRICS-TELEMETRY] üìà Success rate: {system_metrics['avg_success_rate']:.3f}")
          print(f"[METRICS-TELEMETRY] üö® Alerts: {len(alerts)}")
          
          EOF

      - name: "üìà Generate Performance Dashboard"
        run: |
          echo "üìà Generating performance dashboard..."
          python << 'EOF'
          import json
          import os
          from datetime import datetime
          
          print("[DASHBOARD] üìà Generating Performance Dashboard")
          
          # Load telemetry data
          with open('telemetry/system_metrics.json', 'r') as f:
              telemetry = json.load(f)
          
          # Generate dashboard HTML (simplified)
          dashboard_html = f"""
          <!DOCTYPE html>
          <html>
          <head>
              <title>Trading Bot Metrics Dashboard</title>
              <style>
                  body {{ font-family: Arial, sans-serif; margin: 20px; }}
                  .metric {{ display: inline-block; margin: 10px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                  .healthy {{ background-color: #d4edda; }}
                  .warning {{ background-color: #fff3cd; }}
                  .critical {{ background-color: #f8d7da; }}
              </style>
          </head>
          <body>
              <h1>ü§ñ Trading Bot System Dashboard</h1>
              <p>Last Updated: {telemetry['timestamp']}</p>
              
              <h2>System Health</h2>
              <div class="metric {'healthy' if telemetry['system_metrics']['overall_health'] == 'healthy' else 'warning'}">
                  <strong>Overall Health:</strong> {telemetry['system_metrics']['overall_health'].title()}
              </div>
              
              <div class="metric {'healthy' if telemetry['system_metrics']['utilization_percentage'] < 95 else 'critical'}">
                  <strong>Resource Utilization:</strong> {telemetry['system_metrics']['utilization_percentage']:.1f}%
              </div>
              
              <div class="metric {'healthy' if telemetry['system_metrics']['avg_success_rate'] > 0.95 else 'warning'}">
                  <strong>Success Rate:</strong> {telemetry['system_metrics']['avg_success_rate']:.3f}
              </div>
              
              <h2>Workflow Performance</h2>
              <table border="1" style="border-collapse: collapse; width: 100%;">
                  <tr>
                      <th>Workflow</th>
                      <th>Runtime (min)</th>
                      <th>Success Rate</th>
                      <th>Runs/Day</th>
                      <th>Daily Usage (min)</th>
                  </tr>
          """
          
          for workflow, metrics in telemetry['workflow_metrics'].items():
              status_class = 'healthy' if metrics['success_rate'] > 0.95 else 'warning'
              dashboard_html += f"""
                  <tr class="{status_class}">
                      <td>{workflow}</td>
                      <td>{metrics['avg_runtime_minutes']:.1f}</td>
                      <td>{metrics['success_rate']:.3f}</td>
                      <td>{metrics['runs_per_day']:.1f}</td>
                      <td>{metrics['daily_runtime_minutes']:.1f}</td>
                  </tr>
              """
          
          dashboard_html += """
              </table>
              
              <h2>Alerts</h2>
          """
          
          if telemetry['alerts']:
              for alert in telemetry['alerts']:
                  dashboard_html += f"""
                  <div class="metric {'warning' if alert['severity'] == 'medium' else 'critical'}">
                      <strong>{alert['severity'].upper()}:</strong> {alert['message']}<br>
                      <em>Action: {alert['action_required']}</em>
                  </div>
                  """
          else:
              dashboard_html += '<div class="metric healthy">No active alerts</div>'
          
          dashboard_html += """
              </body>
          </html>
          """
          
          # Save dashboard
          with open('telemetry/dashboard.html', 'w') as f:
              f.write(dashboard_html)
          
          print("[DASHBOARD] ‚úÖ Performance dashboard generated")
          
          EOF

      - name: "üîî Check Alert Conditions"
        run: |
          echo "üîî Checking alert conditions..."
          python << 'EOF'
          import json
          
          print("[ALERTS] üîî Checking Alert Conditions")
          
          # Load telemetry data
          with open('telemetry/system_metrics.json', 'r') as f:
              telemetry = json.load(f)
          
          alerts = telemetry['alerts']
          
          if alerts:
              print(f"[ALERTS] üö® {len(alerts)} active alerts:")
              for alert in alerts:
                  print(f"  {alert['severity'].upper()}: {alert['message']}")
          else:
              print("[ALERTS] ‚úÖ No active alerts - system operating normally")
          
          # Set GitHub output for potential notification
          if alerts:
              high_priority_alerts = [a for a in alerts if a['severity'] == 'high']
              if high_priority_alerts:
                  print("::warning::High priority alerts detected - immediate attention required")
              else:
                  print("::notice::Medium priority alerts detected - monitoring required")
          
          EOF

      - name: "üì¶ Upload Telemetry Data"
        uses: actions/upload-artifact@v4
        with:
          name: telemetry-${{ github.run_number }}
          path: telemetry/
          retention-days: 7

      - name: "‚úÖ Telemetry Summary"
        run: |
          echo "‚úÖ Metrics Telemetry Pipeline Complete"
          echo "üìä Workflow metrics aggregated from 9 consolidated workflows"
          echo "üìà Performance dashboard generated"
          echo "üîî Alert conditions checked"
          echo "üíæ Telemetry data stored for monitoring"
          echo "‚è±Ô∏è Runtime: Under 1 minute budget"