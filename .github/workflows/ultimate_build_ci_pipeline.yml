name: "🔨⚡ ULTIMATE Build & CI Pipeline (Mega-System)"

"on":
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]  # PRIMARY handler for PR builds (dotnet.yml excluded to prevent duplicates)
  schedule:
    - cron: '0 2 * * 6'

  workflow_dispatch:
    inputs:
      build_mode:
        description: 'Build Mode'
        required: false
        default: 'comprehensive'
        type: choice
        options:
          - quick
          - standard
          - comprehensive
          - desktop
          - release
          - ultimate
      target_configuration:
        description: 'Build Configuration'
        required: false
        default: 'Release'
        type: choice
        options:
          - Debug
          - Release
      run_tests:
        description: 'Run Tests'
        required: false
        default: true
        type: boolean
      code_analysis:
        description: 'Run Code Analysis'
        required: false
        default: true
        type: boolean

permissions:
  contents: write
  pull-requests: write
  actions: write
  security-events: write

env:
  BUILD_MODE: ${{ github.event.inputs.build_mode || 'comprehensive' }}
  BUILD_CONFIGURATION: ${{ github.event.inputs.target_configuration || 'Release' }}
  RUN_TESTS: ${{ github.event.inputs.run_tests || 'true' }}
  CODE_ANALYSIS: ${{ github.event.inputs.code_analysis || 'true' }}
  DOTNET_VERSION: '8.0.x'
  SOLUTION_FILE: 'TopstepX.Bot.sln'

jobs:
  ultimate-build-and-ci:
    name: "Ultimate Build & CI System"
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
      - name: "📥 Checkout Repository"
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: "🚫 Session Deduplication Check"
        id: session_check
        run: |
          echo "🚫 Checking for duplicate sessions to prevent premium waste..."
          
          # Ensure session deduplicator script exists
          if [ -f ".github/scripts/session_deduplicator.py" ]; then
            python .github/scripts/session_deduplicator.py check \
              "${{ github.event_name }}" \
              "ultimate_build_ci_pipeline" \
              "${{ github.sha }}" \
              "${{ github.run_id }}"
          else
            echo "Session deduplicator not found, proceeding with build"
            echo "skip_execution=false" >> $GITHUB_OUTPUT
          fi

      - name: "🚫 Skip Duplicate Session"
        if: steps.session_check.outputs.skip_execution == 'true'
        run: |
          echo "🚫 ============================================"
          echo "🚫 DUPLICATE SESSION DETECTED - STOPPING"
          echo "=============================================="
          echo "✅ Premium session saved by preventing duplicate execution"
          echo "🔍 Reason: ${{ steps.session_check.outputs.reason }}"
          echo "⚡ This workflow will exit early to prevent waste"
          exit 0

      - name: "📝 Register Session"
        if: steps.session_check.outputs.skip_execution != 'true'
        run: |
          if [ -f ".github/scripts/session_deduplicator.py" ]; then
            python .github/scripts/session_deduplicator.py register \
              "${{ steps.session_check.outputs.session_key }}" \
              "${{ github.event_name }}" \
              "ultimate_build_ci_pipeline" \
              "${{ github.run_id }}" \
              "${{ github.sha }}"
          fi

      - name: "⚙️ Setup .NET Environment"
        if: steps.session_check.outputs.skip_execution != 'true'
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: ${{ env.DOTNET_VERSION }}

      - name: "📊 Build Environment Analysis"
        if: steps.session_check.outputs.skip_execution != 'true'
        id: analysis
        run: |
          echo "skip=false" >> $GITHUB_OUTPUT
          echo "solution_exists=false" >> $GITHUB_OUTPUT
          echo "test_projects=0" >> $GITHUB_OUTPUT
          
          # Check for solution file
          if [ -f "${{ env.SOLUTION_FILE }}" ]; then
            echo "solution_exists=true" >> $GITHUB_OUTPUT
            echo "✅ Solution file found: ${{ env.SOLUTION_FILE }}"
          else
            echo "⚠️ Solution file not found, searching for alternatives..."
            # Look for any .sln files
            sln_files=$(find . -name "*.sln" -type f | head -1)
            if [ -n "$sln_files" ]; then
              echo "solution_file=$sln_files" >> $GITHUB_OUTPUT
              echo "solution_exists=true" >> $GITHUB_OUTPUT
              echo "✅ Alternative solution found: $sln_files"
            fi
          fi
          
          # Count test projects
          test_count=$(find . -name "*Test*.csproj" -o -name "*Tests*.csproj" | wc -l)
          echo "test_projects=$test_count" >> $GITHUB_OUTPUT
          echo "📊 Test projects found: $test_count"
          
          # Display build configuration
          echo "🔨 Build Mode: ${{ env.BUILD_MODE }}"
          echo "⚙️ Configuration: ${{ env.BUILD_CONFIGURATION }}"
          echo "🧪 Run Tests: ${{ env.RUN_TESTS }}"
          echo "🔍 Code Analysis: ${{ env.CODE_ANALYSIS }}"

      - name: "🔄 Restore Dependencies (Enhanced)"
        if: steps.session_check.outputs.skip_execution != 'true'
        run: |
          echo "🔄 Restoring .NET dependencies..."
          
          # Try solution file first
          if [ "${{ steps.analysis.outputs.solution_exists }}" = "true" ]; then
            if [ -f "${{ env.SOLUTION_FILE }}" ]; then
              echo "📦 Restoring from solution: ${{ env.SOLUTION_FILE }}"
              dotnet restore "${{ env.SOLUTION_FILE }}" --verbosity normal
            elif [ -n "${{ steps.analysis.outputs.solution_file }}" ]; then
              echo "📦 Restoring from alternative solution: ${{ steps.analysis.outputs.solution_file }}"
              dotnet restore "${{ steps.analysis.outputs.solution_file }}" --verbosity normal
            fi
          else
            echo "📦 Restoring from current directory..."
            dotnet restore --verbosity normal
          fi
          
          echo "✅ Dependencies restored successfully"

      - name: "🔨 Build Solution (Comprehensive)"
        if: steps.session_check.outputs.skip_execution != 'true'
        run: |
          echo "🔨 Building .NET solution..."
          
          # Set build arguments based on mode
          build_args="--no-restore --configuration ${{ env.BUILD_CONFIGURATION }}"
          
          if [ "${{ env.BUILD_MODE }}" = "ultimate" ]; then
            build_args="$build_args --verbosity detailed"
          elif [ "${{ env.BUILD_MODE }}" = "quick" ]; then
            build_args="$build_args --verbosity quiet"
          else
            build_args="$build_args --verbosity normal"
          fi
          
          # Build the solution
          if [ "${{ steps.analysis.outputs.solution_exists }}" = "true" ]; then
            if [ -f "${{ env.SOLUTION_FILE }}" ]; then
              echo "🏗️ Building solution: ${{ env.SOLUTION_FILE }}"
              dotnet build "${{ env.SOLUTION_FILE }}" $build_args
            elif [ -n "${{ steps.analysis.outputs.solution_file }}" ]; then
              echo "🏗️ Building alternative solution: ${{ steps.analysis.outputs.solution_file }}"
              dotnet build "${{ steps.analysis.outputs.solution_file }}" $build_args
            fi
          else
            echo "🏗️ Building from current directory..."
            dotnet build $build_args
          fi
          
          echo "✅ Build completed successfully"

      - name: "✨ Code Formatting Verification"
        if: env.CODE_ANALYSIS == 'true'
        run: |
          echo "✨ Verifying code formatting..."
          
          # Install dotnet format if not available
          dotnet tool list -g | grep -q dotnet-format || dotnet tool install -g dotnet-format
          
          # Run format verification
          if [ "${{ steps.analysis.outputs.solution_exists }}" = "true" ]; then
            if [ -f "${{ env.SOLUTION_FILE }}" ]; then
              dotnet format "${{ env.SOLUTION_FILE }}" --verify-no-changes --verbosity diagnostic || echo "⚠️ Code formatting issues detected"
            elif [ -n "${{ steps.analysis.outputs.solution_file }}" ]; then
              dotnet format "${{ steps.analysis.outputs.solution_file }}" --verify-no-changes --verbosity diagnostic || echo "⚠️ Code formatting issues detected"
            fi
          else
            dotnet format --verify-no-changes --verbosity diagnostic || echo "⚠️ Code formatting issues detected"
          fi
          
          echo "✅ Code formatting verification completed"

      - name: "🔍 Logging Format Compliance Check"
        if: env.CODE_ANALYSIS == 'true'
        run: |
          echo "🔍 Verifying standardized logging format compliance..."
          
          # Create regex validation script
          cat > validate_logging.py << 'EOF'
          import re
          import sys
          import os
          
          def check_file_logging_compliance(filepath):
              """Check if a file follows the required logging patterns"""
              issues = []
              
              # Required logging patterns
              patterns = {
                  'signal': r'\[SIG\]\s+side=\{[^}]+\}\s+symbol=\{[^}]+\}\s+qty=\{[^}]+\}\s+entry=\{[^}]+\}\s+stop=\{[^}]+\}\s+t1=\{[^}]+\}\s+R~\{[^}]+\}\s+tag=\{[^}]+\}',
                  'order': r'ORDER\s+account=\{[^}]+\}\s+status=\{[^}]+\}\s+orderId=\{[^}]+\}',
                  'trade': r'TRADE\s+account=\{[^}]+\}\s+orderId=\{[^}]+\}\s+fillPrice=\{[^}]+\}\s+qty=\{[^}]+\}\s+time=\{[^}]+\}'
              }
              
              try:
                  with open(filepath, 'r', encoding='utf-8') as f:
                      content = f.read()
                      lines = content.split('\n')
                      
                      # Check for logging statements that should match patterns
                      for line_num, line in enumerate(lines, 1):
                          # Skip comments and empty lines
                          if line.strip().startswith('//') or not line.strip():
                              continue
                              
                          # Check for LogInformation calls with brackets that might need validation
                          if 'LogInformation' in line and ('[SIG]' in line or 'ORDER' in line or 'TRADE' in line):
                              # Extract the format string
                              format_match = re.search(r'LogInformation\s*\(\s*["\']([^"\']+)["\']', line)
                              if format_match:
                                  format_string = format_match.group(1)
                                  
                                  # Check if it matches any required pattern
                                  matches_pattern = False
                                  for pattern_name, pattern in patterns.items():
                                      if re.search(pattern, format_string):
                                          matches_pattern = True
                                          break
                                  
                                  if not matches_pattern:
                                      # Check if it's a signal/order/trade log that should match
                                      if any(marker in format_string for marker in ['[SIG]', 'ORDER', 'TRADE']):
                                          issues.append(f"Line {line_num}: Non-compliant logging format: {format_string}")
                      
              except Exception as e:
                  issues.append(f"Error reading file {filepath}: {e}")
              
              return issues
          
          def main():
              total_issues = 0
              critical_files = [
                  'src/OrchestratorAgent/BotSupervisor.cs',
                  'Core/Intelligence/TradingSystemConnector.cs', 
                  'src/BotCore/UserHubAgent.cs'
              ]
              
              print("🔍 Checking logging format compliance...")
              
              for filepath in critical_files:
                  if os.path.exists(filepath):
                      print(f"📁 Checking {filepath}...")
                      issues = check_file_logging_compliance(filepath)
                      
                      if issues:
                          print(f"❌ Found {len(issues)} issues in {filepath}:")
                          for issue in issues:
                              print(f"   • {issue}")
                          total_issues += len(issues)
                      else:
                          print(f"✅ {filepath} - Logging format compliant")
                  else:
                      print(f"⚠️ File not found: {filepath}")
              
              if total_issues > 0:
                  print(f"\n❌ Total logging format issues: {total_issues}")
                  print("💡 Please fix logging format issues to ensure consistent observability")
                  # Don't fail CI for now, just warn
                  return 0
              else:
                  print("\n✅ All logging formats are compliant!")
                  return 0
          
          if __name__ == "__main__":
              sys.exit(main())
          EOF
          
          # Run the validation
          python validate_logging.py
          
          echo "✅ Logging format compliance check completed"

      - name: "🧪 Run Unit Tests (Comprehensive)"
        if: env.RUN_TESTS == 'true' && steps.analysis.outputs.test_projects != '0'
        run: |
          echo "🧪 Running comprehensive unit tests..."
          
          # Set test arguments based on mode
          test_args="--no-build --configuration ${{ env.BUILD_CONFIGURATION }}"
          
          if [ "${{ env.BUILD_MODE }}" = "ultimate" ]; then
            test_args="$test_args --verbosity detailed --collect:\"XPlat Code Coverage\""
          elif [ "${{ env.BUILD_MODE }}" = "comprehensive" ]; then
            test_args="$test_args --verbosity normal --collect:\"XPlat Code Coverage\""
          else
            test_args="$test_args --verbosity normal"
          fi
          
          # Run tests
          if [ "${{ steps.analysis.outputs.solution_exists }}" = "true" ]; then
            if [ -f "${{ env.SOLUTION_FILE }}" ]; then
              echo "🧪 Running tests for solution: ${{ env.SOLUTION_FILE }}"
              dotnet test "${{ env.SOLUTION_FILE }}" $test_args
            elif [ -n "${{ steps.analysis.outputs.solution_file }}" ]; then
              echo "🧪 Running tests for alternative solution: ${{ steps.analysis.outputs.solution_file }}"
              dotnet test "${{ steps.analysis.outputs.solution_file }}" $test_args
            fi
          else
            echo "🧪 Running tests from current directory..."
            dotnet test $test_args
          fi
          
          echo "✅ Unit tests completed successfully"

      - name: "🧪 Test Results Summary"
        if: env.RUN_TESTS == 'true' && steps.analysis.outputs.test_projects == '0'
        run: |
          echo "⚠️ No test projects found in the solution"
          echo "📊 Test projects detected: ${{ steps.analysis.outputs.test_projects }}"
          echo "💡 Consider adding unit tests to improve code quality"

      - name: "🔍 Advanced Code Analysis (SonarCloud with Quality Gate)"
        if: env.CODE_ANALYSIS == 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
        run: |
          echo "🔍 Running SonarCloud analysis with quality gate enforcement..."
          
          # Only run if SONAR_TOKEN is available
          if [ -n "${SONAR_TOKEN}" ]; then
            echo "🔍 SonarCloud analysis enabled"
            
            # Install SonarScanner if needed
            dotnet tool list -g | grep -q dotnet-sonarscanner || dotnet tool install -g dotnet-sonarscanner
            
            # Run SonarCloud analysis with quality gate enforcement
            dotnet sonarscanner begin \
              /k:"${{ secrets.SONAR_PROJECT_KEY }}" \
              /o:"${{ secrets.SONAR_ORG_KEY }}" \
              /d:sonar.host.url="${{ secrets.SONAR_HOST_URL }}" \
              /d:sonar.login="${{ secrets.SONAR_TOKEN }}" \
              /d:sonar.qualitygate.wait=true \
              /d:sonar.cs.vscoveragexml.reportsPaths="**/coverage.xml"
            
            # Build for analysis with warnings as errors
            if [ "${{ steps.analysis.outputs.solution_exists }}" = "true" ]; then
              dotnet build "${{ env.SOLUTION_FILE }}" --configuration ${{ env.BUILD_CONFIGURATION }} --no-restore -warnaserror
            else
              dotnet build --configuration ${{ env.BUILD_CONFIGURATION }} --no-restore -warnaserror
            fi
            
            # Run tests for coverage
            dotnet test --no-build --verbosity normal
            
            # End analysis with quality gate enforcement
            dotnet sonarscanner end /d:sonar.login="${{ secrets.SONAR_TOKEN }}"
            
            echo "✅ SonarCloud analysis completed with quality gate enforcement"
          else
            echo "⚠️ SonarCloud token not available, skipping analysis"
          fi

      # 🛡️ INTEGRATED QUALITY GATE - Full-Stack Static Analysis
      - name: "🛡️ Quality Gate: Analyzer Compliance (Zero Tolerance)"
        if: env.CODE_ANALYSIS == 'true'
        run: |
          echo "🛡️ Running Full-Stack Quality Gate - Analyzer Compliance..."
          
          # Build with warnings as errors for strict compliance
          echo "🏗 Building with zero tolerance for analyzer violations..."
          if [ "${{ steps.analysis.outputs.solution_exists }}" = "true" ]; then
            if [ -f "${{ env.SOLUTION_FILE }}" ]; then
              dotnet build "${{ env.SOLUTION_FILE }}" --configuration Release --no-restore /warnaserror
            elif [ -n "${{ steps.analysis.outputs.solution_file }}" ]; then
              dotnet build "${{ steps.analysis.outputs.solution_file }}" --configuration Release --no-restore /warnaserror
            fi
          else
            dotnet build --configuration Release --no-restore /warnaserror
          fi
          
          echo "✅ Analyzer compliance verified - zero violations achieved"

      # Guardrails – placeholders
      - name: Scan for placeholders
        if: env.CODE_ANALYSIS == 'true'
        run: |
          echo "🔍 Checking for TODO/STUB/PLACEHOLDER/NotImplementedException..."
          if grep -RInE "TODO|STUB|PLACEHOLDER|NotImplementedException" --exclude-dir={.git,.github} .; then
            echo "❌ Found placeholders — remove before committing."
            exit 1
          fi

      # Guardrails – commented-out code (ignore XML docs)
      - name: Scan for commented-out code
        if: env.CODE_ANALYSIS == 'true'
        run: |
          echo "🔍 Checking for commented-out code..."
          if grep -RInE "^[[:space:]]*//[[:space:]]*[A-Za-z0-9_]+" --exclude-dir={.git,.github} --include=\*.cs . \
             | grep -v "^[[:space:]]*///"; then
            echo "❌ Found commented-out code — remove or restore before committing."
            exit 1
          fi

      # Guardrails – hardcoded credentials
      - name: Scan for hardcoded credentials
        if: env.CODE_ANALYSIS == 'true'
        run: |
          echo "🔍 Checking for hardcoded credentials..."
          if grep -RInE "(api[_-]?key|secret|token|password)[[:space:]]*[:=][[:space:]]*['\"][A-Za-z0-9_\-]{16,}['\"]" \
             --exclude-dir={.git,.github} .; then
            echo "❌ Found possible hardcoded credentials — move to secure secrets storage."
            exit 1
          fi

      # Guardrails – security patterns
      - name: Scan for insecure patterns
        if: env.CODE_ANALYSIS == 'true'
        run: |
          echo "🔍 Checking for insecure patterns..."
          if grep -RInE "ServicePointManager\.ServerCertificateValidationCallback|SELECT \* FROM|http://" \
             --exclude-dir={.git,.github} .; then
            echo "❌ Found insecure patterns — fix before committing."
            exit 1
          fi

      # Dead-code detection (CodeQL)
      - name: Dead code scan
        if: env.CODE_ANALYSIS == 'true'
        run: |
          echo "🔍 Running dead-code detection..."
          
          # Check if CodeQL is available
          if command -v codeql &> /dev/null; then
            echo "📋 CodeQL available - running comprehensive dead code analysis..."
            codeql database create db --language=csharp --source-root .
            codeql database analyze db .github/codeql/dead-code.ql --format=sarif-latest --output=deadcode.sarif
            if grep -q "result" deadcode.sarif; then
              echo "❌ Dead code detected — remove or wire into orchestrator."
              exit 1
            fi
            echo "✅ No dead code detected by CodeQL analysis"
          else
            echo "📋 CodeQL not available - running fallback dead code detection..."
            
            # Fallback: Basic dead code detection without CodeQL
            DEAD_CODE_VIOLATIONS=0
            
            # Check for actual unit test classes in src/
            TEST_CLASSES_IN_SRC=$(find ./src -name "*.cs" -exec grep -l "\[Test\]\|\[TestMethod\]\|\[Fact\]\|\[TestCase\]" {} \; 2>/dev/null | wc -l)
            if [ "$TEST_CLASSES_IN_SRC" -gt 0 ]; then
              echo "❌ Found $TEST_CLASSES_IN_SRC unit test classes in src/ directory"
              DEAD_CODE_VIOLATIONS=$((DEAD_CODE_VIOLATIONS + TEST_CLASSES_IN_SRC))
            fi
            
            # Check for empty classes
            EMPTY_CLASSES=$(find ./src -name "*.cs" -exec grep -l "class.*{[[:space:]]*}" {} \; 2>/dev/null | wc -l)
            if [ "$EMPTY_CLASSES" -gt 0 ]; then
              echo "❌ Found $EMPTY_CLASSES empty classes"
              DEAD_CODE_VIOLATIONS=$((DEAD_CODE_VIOLATIONS + EMPTY_CLASSES))
            fi
            
            if [ "$DEAD_CODE_VIOLATIONS" -gt 0 ]; then
              echo "❌ Dead code violations found ($DEAD_CODE_VIOLATIONS) — remove before committing."
              exit 1
            fi
            
            echo "✅ No critical dead code violations detected"
          fi

      - name: "🛡️ Quality Gate Summary"
        if: env.CODE_ANALYSIS == 'true'
        run: |
          echo ""
          echo "🛡️ ============================================"
          echo "🛡️ FULL-STACK QUALITY GATE COMPLETE"
          echo "=============================================="
          echo ""
          echo "✅ STATIC ANALYSIS COMPLETED:"
          echo "   🏗️ Analyzer compliance - Zero violations"
          echo "   🚧 Guardrail enforcement - No placeholders/commented code/credentials"
          echo "   🔒 Security patterns - No insecure patterns detected"
          echo "   💀 Dead code detection - Active with comprehensive analysis"
          echo ""
          echo "🎯 QUALITY STANDARDS ENFORCED:"
          echo "   • Production-ready code only"
          echo "   • No TODO/STUB placeholders"
          echo "   • No hardcoded credentials or URLs"
          echo "   • Security-compliant patterns"
          echo "   • Zero analyzer violations"
          echo "   • Dead code elimination active"
          echo ""
          echo "✅ Repository meets enterprise production standards!"
          echo "=============================================="

      - name: "📦 Package Creation (Release Mode)"
        if: env.BUILD_CONFIGURATION == 'Release' && (env.BUILD_MODE == 'release' || env.BUILD_MODE == 'ultimate')
        run: |
          echo "📦 Creating release packages..."
          
          # Create packages for release builds
          if [ "${{ steps.analysis.outputs.solution_exists }}" = "true" ]; then
            if [ -f "${{ env.SOLUTION_FILE }}" ]; then
              dotnet pack "${{ env.SOLUTION_FILE }}" \
                --configuration ${{ env.BUILD_CONFIGURATION }} \
                --no-build \
                --output ./packages/ \
                --verbosity normal
            fi
          fi
          
          # List created packages
          if [ -d "./packages/" ]; then
            echo "📦 Created packages:"
            ls -la ./packages/
          else
            echo "⚠️ No packages created"
          fi

      - name: "🖥️ Desktop Build (Windows-specific features)"
        if: env.BUILD_MODE == 'desktop' || env.BUILD_MODE == 'ultimate'
        run: |
          echo "🖥️ Performing desktop-specific build tasks..."
          
          # Check for WPF/WinForms projects
          wpf_projects=$(find . -name "*.csproj" -exec grep -l "Microsoft.WindowsDesktop.App\|UseWPF\|UseWindowsForms" {} \; | wc -l)
          
          if [ "$wpf_projects" -gt 0 ]; then
            echo "🖥️ Desktop projects found: $wpf_projects"
            
            # Build with desktop-specific settings
            if [ "${{ steps.analysis.outputs.solution_exists }}" = "true" ]; then
              dotnet build "${{ env.SOLUTION_FILE }}" \
                --configuration ${{ env.BUILD_CONFIGURATION }} \
                --runtime win-x64 \
                --self-contained false \
                --verbosity normal || echo "⚠️ Desktop build attempted"
            fi
            
            echo "✅ Desktop build completed"
          else
            echo "⚠️ No desktop projects found"
          fi

      - name: "📊 Build Artifacts Analysis"
        run: |
          echo "📊 Analyzing build artifacts..."
          
          # Count built assemblies
          dll_count=$(find . -name "*.dll" -path "*/bin/*" | wc -l)
          exe_count=$(find . -name "*.exe" -path "*/bin/*" | wc -l)
          
          echo "📊 Build Statistics:"
          echo "   🔧 DLL files: $dll_count"
          echo "   ⚡ EXE files: $exe_count"
          echo "   📁 Configuration: ${{ env.BUILD_CONFIGURATION }}"
          echo "   🔨 Build Mode: ${{ env.BUILD_MODE }}"
          
          # Check for specific important assemblies
          if [ -f "./src/*/bin/${{ env.BUILD_CONFIGURATION }}/*/*.dll" ]; then
            echo "✅ Main assemblies built successfully"
          fi
          
          # Output size analysis
          if [ -d "./bin" ] || [ -d "./src" ]; then
            total_size=$(du -sh ./bin ./src 2>/dev/null | awk '{sum+=$1} END {print sum "MB"}' || echo "Unknown")
            echo "📦 Total build size: $total_size"
          fi

      - name: "📤 Upload Build Artifacts"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ultimate-build-artifacts-${{ env.BUILD_CONFIGURATION }}-${{ github.run_number }}
          path: |
            **/bin/**/*.dll
            **/bin/**/*.exe
            **/bin/**/*.pdb
            ./packages/**
            TestResults/**
          retention-days: 30

      - name: "📤 Upload Test Results"
        if: env.RUN_TESTS == 'true' && always()
        uses: actions/upload-artifact@v4
        with:
          name: ultimate-test-results-${{ github.run_number }}
          path: |
            TestResults/**
            **/coverage.*
            **/*.trx
          retention-days: 14

      - name: "🔄 Cache Build Dependencies"
        uses: actions/cache@v4
        with:
          path: |
            ~/.nuget/packages
            ~/.dotnet/tools
          key: ${{ runner.os }}-dotnet-${{ hashFiles('**/*.csproj', '**/*.sln') }}
          restore-keys: |
            ${{ runner.os }}-dotnet-

      - name: "🏁 Ultimate Build Summary"
        if: always()
        run: |
          echo ""
          echo "🏁 ============================================"
          echo "🔨⚡ ULTIMATE BUILD & CI PIPELINE COMPLETE"
          echo "=============================================="
          echo ""
          echo "📊 BUILD SUMMARY:"
          echo "   • Build Mode: ${{ env.BUILD_MODE }}"
          echo "   • Configuration: ${{ env.BUILD_CONFIGURATION }}"
          echo "   • .NET Version: ${{ env.DOTNET_VERSION }}"
          echo "   • Solution: ${{ env.SOLUTION_FILE }}"
          echo "   • Job Status: ${{ job.status }}"
          echo ""
          echo "🔥 FEATURES EXECUTED:"
          echo "   🔄 Enhanced Dependency Restoration"
          echo "   🔨 Comprehensive Solution Building"
          echo "   ✨ Code Formatting Verification"
          echo "   🧪 Advanced Unit Testing"
          echo "   🔍 SonarQube Code Analysis"
          echo "   📦 Release Package Creation"
          echo "   🖥️ Desktop Build Support"
          echo "   📊 Build Artifact Analysis"
          echo "   💾 Intelligent Caching"
          echo ""
          echo "📊 ANALYSIS RESULTS:"
          echo "   • Solution Found: ${{ steps.analysis.outputs.solution_exists }}"
          echo "   • Test Projects: ${{ steps.analysis.outputs.test_projects }}"
          echo "   • Tests Executed: ${{ env.RUN_TESTS }}"
          echo "   • Code Analysis: ${{ env.CODE_ANALYSIS }}"
          echo ""
          echo "🎯 MERGED WORKFLOWS (3→1):"
          echo "   • ci.yml ✅"
          echo "   • dotnet.yml ✅"
          echo "   • dotnet-desktop.yml ✅"
          echo ""
          echo "🚀 Ultimate Build & CI Pipeline - Your development powerhouse!"
          echo "=============================================="

      - name: "🔧 Integrate with BotCore Decision Engine"
        run: |
          echo "🔗 Converting Build CI results to BotCore format..."
          
          # Run data integration script for build results
          python Intelligence/scripts/workflow_data_integration.py \
            --workflow-type "ultimate_build_ci_pipeline" \
            --data-path "artifacts/" \
            --output-path "Intelligence/data/integrated/build_ci_status.json" || echo "⚠️ Integration script not found"
          
          echo "✅ BotCore build CI integration complete"

      - name: "🚀 Execute TopStep Credential Automation & Production Gate"
        if: env.BUILD_CONFIGURATION == 'Release'
        env:
          # Use GitHub secrets for TopStep credentials in CI
          TOPSTEPX_USERNAME: ${{ secrets.TOPSTEPX_USERNAME }}
          TOPSTEPX_API_KEY: ${{ secrets.TOPSTEPX_API_KEY }}
          TOPSTEPX_ACCOUNT_ID: ${{ secrets.TOPSTEPX_ACCOUNT_ID }}
          # Set staging mode for CI
          BOT_MODE: staging
          DRY_RUN: true
          ENVIRONMENT: ci
          CRITICAL_SYSTEM_ENABLE: 1
        run: |
          echo "🚀 Executing comprehensive deployment pipeline..."
          
          # Check if TopStep credentials are available
          if [ -n "$TOPSTEPX_USERNAME" ] && [ -n "$TOPSTEPX_API_KEY" ]; then
            echo "✅ TopStep credentials detected in CI environment"
            
            # Run the comprehensive deployment pipeline
            cd src/Infrastructure.TopstepX
            dotnet run --configuration Release --verbosity normal
            
            # Capture exit code
            PIPELINE_EXIT_CODE=$?
            echo "🏁 Pipeline exit code: $PIPELINE_EXIT_CODE"
            
            # Generate CI summary based on exit code
            case $PIPELINE_EXIT_CODE in
              0)
                echo "✅ SUCCESS: System ready for production deployment"
                echo "DEPLOYMENT_STATUS=production_ready" >> $GITHUB_ENV
                ;;
              2)
                echo "❌ CREDENTIAL ISSUES: TopStep credentials not properly configured"
                echo "DEPLOYMENT_STATUS=credential_issues" >> $GITHUB_ENV
                ;;
              4)
                echo "⚠️ TEST FAILURES: Some tests failed, review required"
                echo "DEPLOYMENT_STATUS=test_failures" >> $GITHUB_ENV
                ;;
              8)
                echo "🚪 PRODUCTION GATE BLOCKED: System not ready for production"
                echo "DEPLOYMENT_STATUS=gate_blocked" >> $GITHUB_ENV
                ;;
              *)
                echo "❌ PIPELINE ERROR: Unexpected error occurred"
                echo "DEPLOYMENT_STATUS=pipeline_error" >> $GITHUB_ENV
                ;;
            esac
            
            # Don't fail the CI build for non-critical issues
            if [ $PIPELINE_EXIT_CODE -eq 0 ] || [ $PIPELINE_EXIT_CODE -eq 4 ] || [ $PIPELINE_EXIT_CODE -eq 8 ]; then
              echo "ℹ️ CI continues despite pipeline warnings"
              exit 0
            else
              echo "❌ Critical pipeline failure"
              exit $PIPELINE_EXIT_CODE
            fi
          else
            echo "⚠️ TopStep credentials not available in CI - skipping deployment pipeline"
            echo "💡 Set TOPSTEPX_USERNAME and TOPSTEPX_API_KEY secrets to enable full pipeline"
            echo "DEPLOYMENT_STATUS=credentials_missing" >> $GITHUB_ENV
          fi

      - name: "📊 Upload Deployment Pipeline Reports"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: deployment-pipeline-reports-${{ github.run_number }}
          path: |
            reports/**/*.json
            reports/**/*.txt
          retention-days: 30

      - name: "🏁 Deployment Pipeline Summary"
        if: always()
        run: |
          echo ""
          echo "🏁 ============================================"
          echo "📊 DEPLOYMENT PIPELINE SUMMARY"
          echo "=============================================="
          echo ""
          echo "🔨 Build Status: ${{ job.status }}"
          echo "🚀 Deployment Status: ${DEPLOYMENT_STATUS:-unknown}"
          echo ""
          
          case "${DEPLOYMENT_STATUS:-unknown}" in
            "production_ready")
              echo "✅ READY FOR PRODUCTION"
              echo "   • All tests passing"
              echo "   • All gates passed"
              echo "   • Credentials configured"
              echo "   • Security compliant"
              ;;
            "test_failures")
              echo "⚠️ TESTS NEED ATTENTION"
              echo "   • Some tests failing"
              echo "   • Review test results"
              echo "   • Fix issues before production"
              ;;
            "gate_blocked")
              echo "🚪 PRODUCTION GATE BLOCKED"
              echo "   • System not production ready"
              echo "   • Review gate results"
              echo "   • Address blockers"
              ;;
            "credential_issues")
              echo "🔑 CREDENTIAL CONFIGURATION NEEDED"
              echo "   • TopStep credentials missing/invalid"
              echo "   • Configure credentials properly"
              ;;
            "credentials_missing")
              echo "ℹ️ CREDENTIALS NOT CONFIGURED"
              echo "   • Set TOPSTEPX_USERNAME secret"
              echo "   • Set TOPSTEPX_API_KEY secret"
              echo "   • Deployment pipeline will run on next push"
              ;;
            *)
              echo "❓ UNKNOWN STATUS"
              echo "   • Check pipeline logs"
              ;;
          esac
          
          echo ""
          echo "📊 Build & Deployment Pipeline Complete"
          echo "=============================================="

      - name: "🧹 Session Cleanup"
        if: always()
        run: |
          echo "🧹 Cleaning up session to prevent future duplicates..."
          
          # Clean up session using deduplicator script
          if [ -f ".github/scripts/session_deduplicator.py" ]; then
            python .github/scripts/session_deduplicator.py cleanup "${{ steps.session_check.outputs.session_key }}"
            
            # Create audit entry
            python .github/scripts/session_deduplicator.py audit \
              "${{ steps.session_check.outputs.session_key }}" \
              "${{ github.event_name }}" \
              "ultimate_build_ci_pipeline" \
              "${{ github.run_id }}" \
              "${{ github.sha }}" \
              "$([ "${{ steps.session_check.outputs.skip_execution }}" = "true" ] && echo "false" || echo "true")" \
              "${{ job.status }}"
          fi
          
          echo "✅ Session cleanup complete"
