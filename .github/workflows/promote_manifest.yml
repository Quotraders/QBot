name: "🏆 Model Promotion Pipeline"

on:
  workflow_run:
    workflows: ["🧠 Model Training Pipeline"]
    types:
      - completed
  workflow_dispatch:
    inputs:
      model_version:
        description: 'Model version to promote'
        required: true
        type: string

concurrency:
  group: model-promotion
  cancel-in-progress: false

permissions:
  contents: write
  actions: read

env:
  MANIFEST_FILE: "manifests/manifest.json"
  RUNTIME_BUDGET: "240"  # 4 minutes

jobs:
  promote-models:
    name: "Model Promotion"
    runs-on: ubuntu-latest
    timeout-minutes: 4
    if: ${{ github.event.workflow_run.conclusion == 'success' || github.event_name == 'workflow_dispatch' }}
    
    steps:
      - name: "📥 Checkout Repository"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: "🐍 Setup Python Environment"
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: "📦 Install Dependencies"
        run: |
          pip install --upgrade pip
          pip install pandas numpy requests

      - name: "📥 Download Latest Model Artifacts"
        run: |
          echo "📥 Downloading latest model artifacts..."
          
          # Download artifacts from the latest successful training run
          run_id=${{ github.event.workflow_run.id }}
          
          if [ -z "$run_id" ]; then
            echo "No workflow run ID available, using manual input"
            echo "Model version: ${{ github.event.inputs.model_version }}"
          else
            echo "Downloading artifacts from run ID: $run_id"
            # In production, this would download actual artifacts
            mkdir -p downloaded_models
            echo "Simulated model download for validation"
          fi

      - name: "🧪 Validate Model Performance Gates"
        run: |
          echo "🧪 Validating model performance gates..."
          python << 'EOF'
          import json
          import os
          from datetime import datetime
          import random
          
          print("[MODEL-VALIDATION] 🧪 Validating Model Performance Gates")
          
          # Performance gate criteria
          performance_gates = {
              'neural_ucb': {
                  'min_sharpe_ratio': 1.2,
                  'max_drawdown': 0.15,
                  'min_num_trades': 100
              },
              'cvar_ppo': {
                  'min_sharpe_ratio': 1.0,
                  'max_drawdown': 0.20,
                  'min_num_trades': 50
              },
              'regime_detection': {
                  'min_accuracy': 0.65,
                  'min_precision': 0.60
              },
              'execution_quality': {
                  'min_r2_score': 0.40,
                  'max_mae': 0.10
              }
          }
          
          # Simulate model validation results
          validation_results = {}
          all_passed = True
          
          for model_name, gates in performance_gates.items():
              print(f"[MODEL-VALIDATION] Validating {model_name}...")
              
              # Simulate performance metrics (in production, would load actual model results)
              if model_name == 'neural_ucb':
                  metrics = {
                      'sharpe_ratio': random.uniform(1.0, 2.0),
                      'max_drawdown': random.uniform(0.10, 0.25),
                      'num_trades': random.randint(80, 200)
                  }
                  passed = (metrics['sharpe_ratio'] >= gates['min_sharpe_ratio'] and 
                           metrics['max_drawdown'] <= gates['max_drawdown'] and
                           metrics['num_trades'] >= gates['min_num_trades'])
              
              elif model_name == 'cvar_ppo':
                  metrics = {
                      'sharpe_ratio': random.uniform(0.8, 1.8),
                      'max_drawdown': random.uniform(0.12, 0.30),
                      'num_trades': random.randint(40, 100)
                  }
                  passed = (metrics['sharpe_ratio'] >= gates['min_sharpe_ratio'] and 
                           metrics['max_drawdown'] <= gates['max_drawdown'] and
                           metrics['num_trades'] >= gates['min_num_trades'])
              
              elif model_name == 'regime_detection':
                  metrics = {
                      'accuracy': random.uniform(0.55, 0.80),
                      'precision': random.uniform(0.50, 0.75)
                  }
                  passed = (metrics['accuracy'] >= gates['min_accuracy'] and 
                           metrics['precision'] >= gates['min_precision'])
              
              elif model_name == 'execution_quality':
                  metrics = {
                      'r2_score': random.uniform(0.30, 0.60),
                      'mae': random.uniform(0.05, 0.15)
                  }
                  passed = (metrics['r2_score'] >= gates['min_r2_score'] and 
                           metrics['mae'] <= gates['max_mae'])
              
              validation_results[model_name] = {
                  'metrics': metrics,
                  'gates': gates,
                  'passed': passed,
                  'timestamp': datetime.utcnow().isoformat()
              }
              
              if not passed:
                  all_passed = False
              
              status = "✅ PASSED" if passed else "❌ FAILED"
              print(f"[MODEL-VALIDATION] {model_name}: {status}")
              for metric, value in metrics.items():
                  print(f"  {metric}: {value:.4f}")
          
          # Save validation results
          os.makedirs('validation_results', exist_ok=True)
          with open('validation_results/performance_gates.json', 'w') as f:
              json.dump(validation_results, f, indent=2)
          
          # Set output for next step
          with open('validation_results/promotion_decision.txt', 'w') as f:
              f.write('PROMOTE' if all_passed else 'REJECT')
          
          print(f"[MODEL-VALIDATION] Overall result: {'✅ PROMOTE' if all_passed else '❌ REJECT'}")
          
          EOF

      - name: "📋 Update Model Manifest"
        run: |
          echo "📋 Updating model manifest..."
          python << 'EOF'
          import json
          import os
          from datetime import datetime
          import hashlib
          
          print("[MANIFEST-UPDATE] 📋 Updating Model Manifest")
          
          # Check promotion decision
          with open('validation_results/promotion_decision.txt', 'r') as f:
              decision = f.read().strip()
          
          if decision != 'PROMOTE':
              print("[MANIFEST-UPDATE] ❌ Models failed validation gates - no manifest update")
              exit(0)
          
          # Load validation results
          with open('validation_results/performance_gates.json', 'r') as f:
              validation_results = json.load(f)
          
          # Create or update manifest
          os.makedirs('manifests', exist_ok=True)
          
          if os.path.exists('manifests/manifest.json'):
              with open('manifests/manifest.json', 'r') as f:
                  manifest = json.load(f)
          else:
              manifest = {
                  'version': '1.0.0',
                  'models': {},
                  'update_history': []
              }
          
          # Update manifest with new model versions
          current_time = datetime.utcnow().isoformat()
          update_entry = {
              'timestamp': current_time,
              'promotion_id': f"promotion-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}",
              'models_promoted': [],
              'validation_results': validation_results
          }
          
          for model_name in validation_results.keys():
              if validation_results[model_name]['passed']:
                  # Simulate model checksums (in production, would get from actual models)
                  model_checksum = hashlib.sha256(f"{model_name}-{current_time}".encode()).hexdigest()
                  
                  manifest['models'][model_name] = {
                      'version': f"v{datetime.utcnow().strftime('%Y%m%d.%H%M%S')}",
                      'checksum': model_checksum,
                      'promoted_at': current_time,
                      'performance_metrics': validation_results[model_name]['metrics'],
                      'status': 'active'
                  }
                  
                  update_entry['models_promoted'].append({
                      'model': model_name,
                      'version': manifest['models'][model_name]['version'],
                      'checksum': model_checksum
                  })
          
          # Add update to history
          manifest['update_history'].append(update_entry)
          
          # Keep only last 10 updates in history
          manifest['update_history'] = manifest['update_history'][-10:]
          
          # Update manifest version
          manifest['last_updated'] = current_time
          
          # Save updated manifest
          with open('manifests/manifest.json', 'w') as f:
              json.dump(manifest, f, indent=2)
          
          print(f"[MANIFEST-UPDATE] ✅ Manifest updated with {len(update_entry['models_promoted'])} promoted models")
          for model_info in update_entry['models_promoted']:
              print(f"  {model_info['model']}: {model_info['version']} ({model_info['checksum'][:16]}...)")
          
          EOF

      - name: "📤 Upload Promotion Results"
        uses: actions/upload-artifact@v4
        with:
          name: promotion-results-${{ github.run_number }}
          path: |
            validation_results/
            manifests/
          retention-days: 30

      - name: "✅ Promotion Summary"
        run: |
          echo "✅ Model Promotion Pipeline Complete"
          
          # Read promotion decision
          if [ -f "validation_results/promotion_decision.txt" ]; then
            decision=$(cat validation_results/promotion_decision.txt)
            if [ "$decision" = "PROMOTE" ]; then
              echo "🏆 Models PROMOTED - passed all performance gates"
              echo "📋 Manifest updated with new model versions"
              echo "✅ Performance gates validated:"
              echo "  • Sharpe ratio thresholds met"
              echo "  • Maximum drawdown limits respected" 
              echo "  • Minimum trade count achieved"
              echo "  • Model accuracy targets reached"
            else
              echo "❌ Models REJECTED - failed performance gates"
              echo "📋 Manifest NOT updated"
              echo "🔄 Models require retraining or parameter adjustment"
            fi
          else
            echo "⚠️ No promotion decision found"
          fi
          
          echo "⏱️ Runtime: Under 4 minutes budget"