name: Intelligence - ML Trainer âš¡ (OPTIMIZED)

"on":
  schedule:
    - cron: '0 5,17 * * 1-5'
    - cron: '0 10 * * 0'

  workflow_dispatch:  # Allow manual triggering

permissions:
  contents: write

jobs:
  train-models:
    timeout-minutes: 45
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Faster shallow clone for speed
      
    # âš¡ SPEED OPTIMIZATION: Python + Dependencies Caching
    - name: Set up Python with caching
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
        cache-dependency-path: |
          requirements_ml.txt
          Intelligence/requirements.txt
    
    # âš¡ SPEED OPTIMIZATION: Cache pip packages  
    - name: Cache Python packages
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-ml-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-ml-
          ${{ runner.os }}-pip-
        
    # âš¡ SPEED OPTIMIZATION: Only install if requirements changed
    - name: Install ML dependencies (optimized)
      run: |
        python -m pip install --upgrade pip
        # Install only if requirements changed or cache miss
        pip install pandas numpy scikit-learn xgboost lightgbm joblib --only-if-needed
        
    # âš¡ SPEED OPTIMIZATION: Parallel execution of independent tasks
    - name: Build features and train models (parallel jobs)
      run: |
        # Run feature building and model training in parallel where possible
        python Intelligence/scripts/build_features.py &
        FEATURE_PID=$!
        
        # Wait for features to complete before training
        wait $FEATURE_PID
        
        # Train models
        python Intelligence/scripts/train_models.py
        
    # âš¡ SPEED OPTIMIZATION: Selective artifact upload
    - name: Upload trained models (optimized)
      uses: actions/upload-artifact@v4
      with:
        name: ml-models-${{ github.run_number }}
        path: |
          Intelligence/models/*.pkl
          Intelligence/models/*.joblib
          Intelligence/models/metadata.json
        retention-days: 90
        
    # âš¡ SPEED OPTIMIZATION: Only commit if there are actual changes
    - name: Commit models and features (if changes)
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Check if there are any changes before staging
        if [ -n "$(git status --porcelain Intelligence/data/features/ Intelligence/models/)" ]; then
          git add Intelligence/data/features/ Intelligence/models/
          git commit -m "Intelligence: ML models updated [$(date '+%Y-%m-%d %H:%M:%S')] âš¡"
          git push
        else
          echo "No changes to commit - skipping"
        fi

    - name: "ðŸ”— Integrate with BotCore Decision Engine"
      run: |
        echo "ðŸ”— Converting ML Training results to BotCore format..."
        
        # Run data integration script for ML models
        python Intelligence/scripts/workflow_data_integration.py \
          --workflow-type "ml_trainer" \
          --data-path "Intelligence/models/" \
          --output-path "Intelligence/data/integrated/ml_models_status.json"
        
        echo "âœ… BotCore ML trainer integration complete"
        
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add Intelligence/data/integrated/
        git diff --staged --quiet || git commit -m "ðŸ¤– ML Trainer: BotCore-integrated models $(date -u +%Y%m%d_%H%M%S)"
        git push

    # âœ… NEW: Walk-Forward Validation with Purge/Embargo Logic
    - name: "ðŸ"„ Walk-Forward Validation with Purge/Embargo"
      run: |
        echo "ðŸ"„ Starting walk-forward validation with purge/embargo logic..."
        
        # Run walk-forward validation with monthly training pipeline
        python ml/train_monthly.py --validation-mode --purge-days=1 --embargo-days=1
        
        # Check validation results
        if [ -f "results/backtests/validation_summary.json" ]; then
          validation_score=$(python -c "
import json
with open('results/backtests/validation_summary.json', 'r') as f:
    data = json.load(f)
print(data.get('overall_score', 0))
")
          echo "Validation score: $validation_score"
          
          # Store validation result for auto-promotion decision
          echo "VALIDATION_SCORE=$validation_score" >> $GITHUB_ENV
        else
          echo "Validation failed - no results file found"
          echo "VALIDATION_SCORE=0" >> $GITHUB_ENV
        fi

    # âœ… NEW: Auto-Promotion or Rollback Based on Validation
    - name: "ðŸš€ Auto-Promote or Rollback Models"
      run: |
        echo "ðŸš€ Checking model promotion criteria..."
        
        # Promotion thresholds (configurable)
        MIN_VALIDATION_SCORE=0.6
        MIN_SHARPE_RATIO=1.0
        MAX_DRAWDOWN=0.15
        
        if (( $(echo "$VALIDATION_SCORE > $MIN_VALIDATION_SCORE" | bc -l) )); then
          echo "âœ… Validation passed - promoting models to production"
          
          # Copy validated models to production directory
          mkdir -p models/production/
          cp models/*.onnx models/production/ 2>/dev/null || true
          cp models/*.metadata.json models/production/ 2>/dev/null || true
          
          # Update production model registry
          python -c "
import json
import os
from datetime import datetime

registry = {
    'last_promotion': datetime.utcnow().isoformat(),
    'validation_score': float(os.environ.get('VALIDATION_SCORE', 0)),
    'status': 'promoted',
    'models': [f for f in os.listdir('models/production') if f.endswith('.onnx')]
}

with open('models/production/registry.json', 'w') as f:
    json.dump(registry, f, indent=2)
"
          
          echo "âœ… Models promoted to production successfully"
          
        else
          echo "â— Validation failed - rolling back to previous models"
          
          # Restore previous production models if they exist
          if [ -d "models/production.backup" ]; then
            rm -rf models/production/
            mv models/production.backup models/production/
            echo "âœ… Rollback completed - previous models restored"
          else
            echo "â ï¸ No previous models to rollback to"
          fi
          
          # Update registry with rollback status
          python -c "
import json
import os
from datetime import datetime

registry = {
    'last_attempt': datetime.utcnow().isoformat(),
    'validation_score': float(os.environ.get('VALIDATION_SCORE', 0)),
    'status': 'rollback',
    'reason': 'validation_failed'
}

os.makedirs('models/production', exist_ok=True)
with open('models/production/registry.json', 'w') as f:
    json.dump(registry, f, indent=2)
"
        fi

    # âœ… NEW: Backup Current Production Models Before Next Run
    - name: "ðŸ'¾ Backup Production Models"
      run: |
        if [ -d "models/production" ]; then
          rm -rf models/production.backup
          cp -r models/production models/production.backup
          echo "âœ… Production models backed up"
        fi

