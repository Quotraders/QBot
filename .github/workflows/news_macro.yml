name: "ğŸ“° News Macro Pipeline"

on:
  schedule:
    - cron: '15 9,10,11,12,13,15 * * 1-5'  # 6 times daily: 9:15 AM, 10:15 AM, 11:15 AM, 12:15 PM, 1:15 PM, 3:15 PM ET
  workflow_dispatch:
    inputs:
      analysis_mode:
        description: 'Analysis mode'
        required: false
        default: 'macro_focus'
        type: choice
        options:
          - macro_focus
          - comprehensive

concurrency:
  group: news-macro
  cancel-in-progress: true

permissions:
  contents: write
  actions: read

env:
  NEWS_FLAGS_DIR: "datasets/news_flags"
  RUNTIME_BUDGET: "180"  # 3 minutes

jobs:
  news-macro-analysis:
    name: "News Macro Analysis"
    runs-on: ubuntu-latest
    timeout-minutes: 3
    
    steps:

    
      - name: "ğŸ”§ Enable Long Paths"

    
        run: git config --global core.longpaths true

    
        

    
      - name:steps:
      - name: "ğŸ“¥ Checkout Repository"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: "ğŸ Setup Python Environment"
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: "ğŸ“¦ Install Dependencies"
        run: |
          pip install --upgrade pip
          pip install feedparser requests beautifulsoup4
          pip install pandas numpy pyarrow
          pip install textblob nltk
          python -c "import nltk; nltk.download('punkt'); nltk.download('vader_lexicon')"

      - name: "ğŸ“° GDELT Macro Economic Analysis"
        run: |
          echo "ğŸ“° Collecting GDELT macro economic data..."
          python << 'EOF'
          import requests
          import pandas as pd
          import json
          import os
          from datetime import datetime
          
          print("[GDELT-MACRO] ğŸ“° GDELT Macro Economic Analysis")
          
          # Macro economic keywords focused on ES/NQ futures drivers
          macro_keywords = [
              "Federal Reserve", "FOMC", "Jerome Powell", "interest rates", "monetary policy",
              "CPI", "inflation", "NFP", "unemployment", "GDP", "recession",
              "market volatility", "VIX", "economic growth", "yield curve",
              "Treasury", "dollar index", "DXY", "bond yields", "liquidity"
          ]
          
          gdelt_data = []
          
          for keyword in macro_keywords[:5]:  # Limit for runtime budget
              try:
                  # GDELT API for financial events
                  base_url = "https://api.gdeltproject.org/api/v2/doc/doc"
                  params = {
                      'query': f'"{keyword}"',
                      'mode': 'ArtList',
                      'maxrecords': 10,
                      'format': 'json',
                      'sortby': 'DateDesc'
                  }
                  
                  print(f"[GDELT-MACRO] Searching for: {keyword}")
                  response = requests.get(base_url, params=params, timeout=10)
                  
                  if response.status_code == 200:
                      try:
                          data = response.json()
                          articles = data.get('articles', [])
                          
                          for article in articles[:3]:  # Limit per keyword
                              event_flag = {
                                  'timestamp': datetime.utcnow().isoformat(),
                                  'keyword': keyword,
                                  'title': article.get('title', ''),
                                  'url': article.get('url', ''),
                                  'source': article.get('domain', ''),
                                  'date': article.get('seendate', ''),
                                  'event_type': 'macro_economic',
                                  'content_usage': 'full_content_allowed',
                                  'es_nq_relevance': 'high' if keyword in ['Federal Reserve', 'FOMC', 'inflation', 'unemployment'] else 'medium'
                              }
                              gdelt_data.append(event_flag)
                              
                      except json.JSONDecodeError:
                          print(f"[GDELT-MACRO] {keyword}: Invalid JSON response")
                  
              except Exception as e:
                  print(f"[GDELT-MACRO] {keyword} error: {e}")
          
          # Save GDELT data as parquet
          os.makedirs('datasets/news_flags', exist_ok=True)
          
          if gdelt_data:
              df = pd.DataFrame(gdelt_data)
              df.to_parquet('datasets/news_flags/gdelt_macro_events.parquet', index=False)
              print(f"[GDELT-MACRO] âœ… Saved {len(gdelt_data)} macro events")
          else:
              # Create empty DataFrame with schema
              df = pd.DataFrame(columns=['timestamp', 'keyword', 'title', 'url', 'source', 'date', 'event_type', 'content_usage', 'es_nq_relevance'])
              df.to_parquet('datasets/news_flags/gdelt_macro_events.parquet', index=False)
              print("[GDELT-MACRO] âš ï¸ No data retrieved, saved empty schema")
          
          EOF

      - name: "ğŸ“„ Commercial Headlines Collection (Compliance)"
        run: |
          echo "ğŸ“„ Collecting commercial headlines (compliance mode)..."
          python << 'EOF'
          import feedparser
          import pandas as pd
          import json
          import os
          from datetime import datetime
          from textblob import TextBlob
          
          print("[COMMERCIAL-HEADLINES] ğŸ“„ Commercial Headlines Collection")
          
          # Commercial sources - headlines only for compliance
          commercial_feeds = {
              'marketwatch': 'https://feeds.marketwatch.com/marketwatch/topstories/',
              'cnbc': 'https://search.cnbc.com/rs/search/combined.do?partnerId=wrss01&id=15839069'
          }
          
          headlines_data = []
          
          for source_name, feed_url in commercial_feeds.items():
              try:
                  print(f"[COMMERCIAL-HEADLINES] Processing {source_name}...")
                  feed = feedparser.parse(feed_url)
                  
                  for entry in feed.entries[:10]:  # Limit entries
                      title = entry.get('title', '')
                      link = entry.get('link', '')
                      published = entry.get('published', '')
                      
                      # Headlines only - no full content for compliance
                      try:
                          sentiment = TextBlob(title.lower()).sentiment.polarity
                      except:
                          sentiment = 0.0
                      
                      # Check for macro relevance
                      macro_terms = ['fed', 'federal reserve', 'inflation', 'unemployment', 'gdp', 'recession', 'fomc', 'powell']
                      is_macro_relevant = any(term in title.lower() for term in macro_terms)
                      
                      if is_macro_relevant:  # Only store macro-relevant headlines
                          headline_flag = {
                              'timestamp': datetime.utcnow().isoformat(),
                              'source': source_name,
                              'title': title[:200],  # Truncate for storage efficiency
                              'url': link,
                              'published': published,
                              'sentiment': sentiment,
                              'event_type': 'commercial_headline',
                              'content_usage': 'headlines_only',
                              'macro_relevance': 'high' if any(term in title.lower() for term in ['fed', 'powell', 'inflation', 'unemployment']) else 'medium',
                              'es_nq_impact': 'potential'
                          }
                          headlines_data.append(headline_flag)
                  
                  print(f"[COMMERCIAL-HEADLINES] {source_name}: Processed {len([h for h in headlines_data if h['source'] == source_name])} macro headlines")
                  
              except Exception as e:
                  print(f"[COMMERCIAL-HEADLINES] Error processing {source_name}: {e}")
          
          # Save commercial headlines as parquet
          if headlines_data:
              df = pd.DataFrame(headlines_data)
              df.to_parquet('datasets/news_flags/commercial_headlines.parquet', index=False)
              print(f"[COMMERCIAL-HEADLINES] âœ… Saved {len(headlines_data)} macro-relevant headlines")
          else:
              # Create empty DataFrame with schema
              df = pd.DataFrame(columns=['timestamp', 'source', 'title', 'url', 'published', 'sentiment', 'event_type', 'content_usage', 'macro_relevance', 'es_nq_impact'])
              df.to_parquet('datasets/news_flags/commercial_headlines.parquet', index=False)
              print("[COMMERCIAL-HEADLINES] âš ï¸ No macro-relevant headlines found")
          
          EOF

      - name: "ğŸ›ï¸ Government Source Collection (Full Content)"
        run: |
          echo "ğŸ›ï¸ Collecting government source data..."
          python << 'EOF'
          import feedparser
          import pandas as pd
          import json
          import os
          from datetime import datetime
          from textblob import TextBlob
          
          print("[GOVERNMENT-SOURCES] ğŸ›ï¸ Government Source Collection")
          
          # Government sources - full content allowed
          government_feeds = {
              'federal_reserve': 'https://www.federalreserve.gov/feeds/press_all.xml',
              'bureau_labor_stats_employment': 'https://www.bls.gov/feed/news_release/empsit.rss',
              'bureau_labor_stats_cpi': 'https://www.bls.gov/feed/news_release/cpi.rss'
          }
          
          government_data = []
          
          for source_name, feed_url in government_feeds.items():
              try:
                  print(f"[GOVERNMENT-SOURCES] Processing {source_name}...")
                  feed = feedparser.parse(feed_url)
                  
                  for entry in feed.entries[:5]:  # Limit entries for budget
                      title = entry.get('title', '')
                      summary = entry.get('summary', '')
                      link = entry.get('link', '')
                      published = entry.get('published', '')
                      
                      # Full content allowed for government sources
                      full_content = f"{title} {summary}"
                      
                      try:
                          sentiment = TextBlob(full_content.lower()).sentiment.polarity
                      except:
                          sentiment = 0.0
                      
                      # Determine ES/NQ impact
                      high_impact_terms = ['monetary policy', 'interest rate', 'inflation', 'employment', 'unemployment', 'fomc']
                      es_nq_impact = 'high' if any(term in full_content.lower() for term in high_impact_terms) else 'medium'
                      
                      government_event = {
                          'timestamp': datetime.utcnow().isoformat(),
                          'source': source_name,
                          'title': title,
                          'summary': summary[:500],  # Limit summary length
                          'url': link,
                          'published': published,
                          'sentiment': sentiment,
                          'event_type': 'government_release',
                          'content_usage': 'full_content_allowed',
                          'es_nq_impact': es_nq_impact,
                          'authority_level': 'official'
                      }
                      government_data.append(government_event)
                  
                  print(f"[GOVERNMENT-SOURCES] {source_name}: Processed {len([g for g in government_data if g['source'] == source_name])} releases")
                  
              except Exception as e:
                  print(f"[GOVERNMENT-SOURCES] Error processing {source_name}: {e}")
          
          # Save government data as parquet
          if government_data:
              df = pd.DataFrame(government_data)
              df.to_parquet('datasets/news_flags/government_releases.parquet', index=False)
              print(f"[GOVERNMENT-SOURCES] âœ… Saved {len(government_data)} government releases")
          else:
              # Create empty DataFrame with schema
              df = pd.DataFrame(columns=['timestamp', 'source', 'title', 'summary', 'url', 'published', 'sentiment', 'event_type', 'content_usage', 'es_nq_impact', 'authority_level'])
              df.to_parquet('datasets/news_flags/government_releases.parquet', index=False)
              print("[GOVERNMENT-SOURCES] âš ï¸ No government releases found")
          
          EOF

      - name: "ğŸ“… Scrape ForexFactory Economic Calendar"
        run: |
          python3 << 'EOF'
          import requests
          from bs4 import BeautifulSoup
          import json
          from datetime import datetime, timedelta
          import os
          
          print("[FOREX-FACTORY] ğŸ“… Scraping economic calendar...")
          
          try:
              # Create output directory
              os.makedirs("datasets/economic_calendar", exist_ok=True)
              
              # Get current week's calendar
              url = "https://www.forexfactory.com/calendar"
              headers = {
                  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
              }
              
              response = requests.get(url, headers=headers, timeout=10)
              response.raise_for_status()
              
              soup = BeautifulSoup(response.content, 'html.parser')
              
              # Find calendar table
              calendar_table = soup.find('table', class_='calendar__table')
              
              if not calendar_table:
                  print("[FOREX-FACTORY] âš ï¸ Calendar table not found, using fallback events")
                  # Create fallback events for testing
                  events = [
                      {
                          "date": (datetime.utcnow() + timedelta(days=7)).strftime("%Y-%m-%d"),
                          "time": "14:00",
                          "currency": "USD",
                          "impact": "High",
                          "event": "FOMC Interest Rate Decision",
                          "forecast": "",
                          "previous": ""
                      }
                  ]
              else:
                  events = []
                  current_date = None
                  
                  # Parse calendar rows
                  rows = calendar_table.find_all('tr', class_='calendar__row')
                  
                  for row in rows[:50]:  # Limit to first 50 events
                      # Extract date
                      date_cell = row.find('td', class_='calendar__date')
                      if date_cell and date_cell.text.strip():
                          date_text = date_cell.text.strip()
                          try:
                              current_date = datetime.strptime(date_text, "%a%b %d").replace(year=datetime.now().year)
                          except:
                              pass
                      
                      # Extract time
                      time_cell = row.find('td', class_='calendar__time')
                      time_text = time_cell.text.strip() if time_cell else ""
                      
                      # Extract currency
                      currency_cell = row.find('td', class_='calendar__currency')
                      currency = currency_cell.text.strip() if currency_cell else ""
                      
                      # Extract impact
                      impact_cell = row.find('td', class_='calendar__impact')
                      impact_span = impact_cell.find('span') if impact_cell else None
                      impact = "Low"
                      if impact_span:
                          if 'icon--ff-impact-red' in impact_span.get('class', []):
                              impact = "High"
                          elif 'icon--ff-impact-ora' in impact_span.get('class', []):
                              impact = "Medium"
                      
                      # Only keep High and Medium impact events
                      if impact not in ["High", "Medium"]:
                          continue
                      
                      # Extract event name
                      event_cell = row.find('td', class_='calendar__event')
                      event_name = event_cell.text.strip() if event_cell else ""
                      
                      # Extract forecast and previous
                      forecast_cell = row.find('td', class_='calendar__forecast')
                      forecast = forecast_cell.text.strip() if forecast_cell else ""
                      
                      previous_cell = row.find('td', class_='calendar__previous')
                      previous = previous_cell.text.strip() if previous_cell else ""
                      
                      if current_date and event_name and currency:
                          events.append({
                              "date": current_date.strftime("%Y-%m-%d"),
                              "time": time_text or "00:00",
                              "currency": currency,
                              "impact": impact,
                              "event": event_name,
                              "forecast": forecast,
                              "previous": previous
                          })
              
              # Save to JSON
              output_path = "datasets/economic_calendar/calendar.json"
              with open(output_path, 'w') as f:
                  json.dump(events, f, indent=2)
              
              print(f"[FOREX-FACTORY] âœ… Collected {len(events)} high/medium impact events")
              
              # Print sample events
              for event in events[:5]:
                  print(f"  ğŸ“… {event['date']} {event['time']} - {event['currency']} - {event['impact']} - {event['event']}")
              
          except requests.exceptions.RequestException as e:
              print(f"[FOREX-FACTORY] âš ï¸ Network error: {e}")
              print("[FOREX-FACTORY] Creating minimal fallback calendar")
              
              # Create minimal fallback
              events = [
                  {
                      "date": (datetime.utcnow() + timedelta(days=7)).strftime("%Y-%m-%d"),
                      "time": "14:00",
                      "currency": "USD",
                      "impact": "High",
                      "event": "FOMC Interest Rate Decision",
                      "forecast": "",
                      "previous": ""
                  }
              ]
              
              os.makedirs("datasets/economic_calendar", exist_ok=True)
              with open("datasets/economic_calendar/calendar.json", 'w') as f:
                  json.dump(events, f, indent=2)
          
          except Exception as e:
              print(f"[FOREX-FACTORY] âŒ Error: {e}")
              raise
          
          EOF

      - name: "ğŸ’¾ Commit Economic Calendar"
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add datasets/economic_calendar/ || true
          git diff --cached --quiet || git commit -m "Update economic calendar from ForexFactory [skip ci]" || true
          git push || true

      - name: "ğŸ“¦ Upload News Flag Artifacts"
        uses: actions/upload-artifact@v4
        with:
          name: news-flags-${{ github.run_number }}
          path: datasets/news_flags/
          retention-days: 7

      - name: "âœ… News Macro Summary"
        run: |
          echo "âœ… News Macro Pipeline Complete"
          echo "ğŸ“° GDELT macro economic events collected"
          echo "ğŸ“„ Commercial headlines (MarketWatch, CNBC) - compliance mode"
          echo "ğŸ›ï¸ Government sources (Fed, BLS) - full content"
          echo "ğŸ“… ForexFactory economic calendar scraped"
          echo "ğŸš« Bloomberg/Reuters/Seeking Alpha - REMOVED"
          echo "ğŸš« Individual stocks (Tesla, Nvidia) - REMOVED"
          echo "ğŸš« Bitcoin/crypto terms - REMOVED"
          echo "ğŸ’¾ All outputs saved as parquet event flags"
          echo "â±ï¸ Runtime: Under 3 minutes budget"